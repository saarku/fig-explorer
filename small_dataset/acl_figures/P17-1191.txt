<figure>1</figure>
<caption>Figure 1: An example grounding for the word ‘pool’. Solid arrows represent possible senses and dashed arrows represent hypernym relations. Note that the same set of concepts are used to ground the word ‘pool’ regardless of its context. Other WordNet senses for ‘pool’ were removed from the figure for simplicity.</caption>
<mention10>"... to related words which share one or more concept embeddings. Figure 1: An example grounding for the word 'pool'. Solid arrows ...  A simplified grounding of the word 'pool' is illustrated in Figure 1. This grounding is key to our model of token ...  hypernyms for a synset s. 1 For example, according to Figure 1: Hypernyms(pond.n.01) = [pond.n.01, lake.n.01, body of water.n.01, entity.n.01] Each ..."</mention10>
<mention20>"... data for a downstream task benefits from all the updates to related words which share one or more concept embeddings. Figure 1: An example grounding for the word 'pool'. Solid arrows represent possible senses and dashed arrows represent hypernym relations. Note ...  and indirect) hypernyms of the WordNet senses of that word. A simplified grounding of the word 'pool' is illustrated in Figure 1. This grounding is key to our model of token embeddings, to be described in the following subsections. Our goal ...  type w in WordNet, and Hypernyms(s) be the list of hypernyms for a synset s. 1 For example, according to Figure 1: Hypernyms(pond.n.01) = [pond.n.01, lake.n.01, body of water.n.01, entity.n.01] Each WordNet synset s is associated with a set of parameters ..."</mention20>
<mention50>"... the embeddings of related words by forcing related words to share similar concept embeddings. As a result, the representation of a rare word which does not appear in the training data for a downstream task benefits from all the updates to related words which share one or more concept embeddings. Figure 1: An example grounding for the word 'pool'. Solid arrows represent possible senses and dashed arrows represent hypernym relations. Note that the same set of concepts are used to ground the word 'pool' regardless of its context. Other WordNet senses for 'pool' were removed from the figure for simplicity. Our ...  to help model generalization and selectional preferences between words, which is especially important for predicting PP attachments (Resnik, 1993). To ground a word type, we identify the set of (direct and indirect) hypernyms of the WordNet senses of that word. A simplified grounding of the word 'pool' is illustrated in Figure 1. This grounding is key to our model of token embeddings, to be described in the following subsections. Our goal is to define a context-sensitive model of token embeddings which can be used as a dropin replacement for traditional type-level word embeddings. Notation. Let Senses(w) be the list of synsets defined as possible word senses of a given word type w in WordNet, and Hypernyms(s) be the list of hypernyms for a synset s. 1 For example, according to Figure 1: Hypernyms(pond.n.01) = [pond.n.01, lake.n.01, body of water.n.01, entity.n.01] Each WordNet synset s is associated with a set of parameters v s ∈ R n which represent its embedding. This parameterization is similar to that of Rothe and Schütze (2015). Embedding model. Given a sequence of tokens t and their ..."</mention50>
<lines3> As a result, the representation of a rare word which does not appear in the training data for a downstream task benefits from all the updates to related words which share one or more concept embeddings. Figure 1: An example grounding for the word 'pool'. Solid arrows represent possible senses and dashed arrows represent hypernym relations. ...  To ground a word type, we identify the set of (direct and indirect) hypernyms of the WordNet senses of that word. A simplified grounding of the word 'pool' is illustrated in Figure 1. This grounding is key to our model of token embeddings, to be described in the following subsections. ...  Notation. Let Senses(w) be the list of synsets defined as possible word senses of a given word type w in WordNet, and Hypernyms(s) be the list of hypernyms for a synset s. 1 For example, according to Figure 1: Hypernyms(pond.n.01) = [pond.n.01, lake.n.01, body of water.n.01, entity.n.01] Each WordNet synset s is associated with a set of parameters v s ∈ R n which represent its embedding. This parameterization is similar to that of Rothe and Schütze (2015)"</lines3>
<lines5> 1n addition to providing context-sensitive token embeddings, the proposed method implicitly regularizes the embeddings of related words by forcing related words to share similar concept embeddings. As a result, the representation of a rare word which does not appear in the training data for a downstream task benefits from all the updates to related words which share one or more concept embeddings. Figure 1: An example grounding for the word 'pool'. Solid arrows represent possible senses and dashed arrows represent hypernym relations. Note that the same set of concepts are used to ground the word 'pool' regardless of its context. ...  Among the labeled relations defined in WordNet between different synsets, we focus on the hypernymy relation to help model generalization and selectional preferences between words, which is especially important for predicting PP attachments (Resnik, 1993). To ground a word type, we identify the set of (direct and indirect) hypernyms of the WordNet senses of that word. A simplified grounding of the word 'pool' is illustrated in Figure 1. This grounding is key to our model of token embeddings, to be described in the following subsections. Our goal is to define a context-sensitive model of token embeddings which can be used as a dropin replacement for traditional type-level word embeddings. Notation. Let Senses(w) be the list of synsets defined as possible word senses of a given word type w in WordNet, and Hypernyms(s) be the list of hypernyms for a synset s. 1 For example, according to Figure 1: Hypernyms(pond.n.01) = [pond.n.01, lake.n.01, body of water.n.01, entity.n.01] Each WordNet synset s is associated with a set of parameters v s ∈ R n which represent its embedding. This parameterization is similar to that of Rothe and Schütze (2015). Embedding model"</lines5>
<snippet3> As a result, the representation of a rare word which does not appear in the training data for a downstream task benefits from all the updates to related words which share one or more concept embeddings. Figure 1: An example grounding for the word 'pool'. Solid arrows represent possible senses and dashed arrows represent hypernym relations.</snippet3>
<snippet5> 1n addition to providing context-sensitive token embeddings, the proposed method implicitly regularizes the embeddings of related words by forcing related words to share similar concept embeddings. As a result, the representation of a rare word which does not appear in the training data for a downstream task benefits from all the updates to related words which share one or more concept embeddings. Figure 1: An example grounding for the word 'pool'. Solid arrows represent possible senses and dashed arrows represent hypernym relations. Note that the same set of concepts are used to ground the word 'pool' regardless of its context.</snippet5>
<abstract>Type-level word embeddings use the same set of parameters to represent all instances of a word regardless of its context, ignoring the inherent lexical ambiguity in language. Instead, we embed semantic concepts (or synsets) as defined in WordNet and represent a word token in a particular context by estimating a distribution over relevant semantic concepts. We use the new, context-sensitive embeddings in a model for predicting prepositional phrase (PP) attachments and jointly learn the concept embeddings and model parameters. We show that using context-sensitive em-beddings improves the accuracy of the PP attachment model by 5.4% absolute points, which amounts to a 34.4% relative reduction in errors.</abstract>
<title>Ontology-Aware Token Embeddings for Prepositional Phrase Attachment</title>
<introduction>Type-level word embeddings map a word type (i.e., a surface form) to a dense vector of real numbers such that similar word types have similar embeddings. When pre-trained on a large corpus of unlabeled text, they provide an effective mechanism for generalizing statistical models to words which do not appear in the labeled training data for a downstream task.In accordance with standard terminology, we make the following distinction between types and tokens in this paper: By word types, we mean the surface form of the word, whereas by tokens we mean the instantiation of the surface form in a context. For example, the same word type 'pool' occurs as two different tokens in the sentences "He sat by the pool," and "He played a game of pool."Most word embedding models define a single vector for each word type. However, a fundamental flaw in this design is their inability to distinguish between different meanings and abstractions of the same word. In the two sentences shown above, the word 'pool' has different meanings, but the same representation is typically used for both of them. Similarly, the fact that 'pool' and 'lake' are both kinds of water bodies is not explicitly incorporated in most type-level embeddings. Furthermore, it has become a standard practice to tune pre-trained word embeddings as model parameters during training for an NLP task (e.g., Chen and Manning, 2014;Lample et al., 2016), potentially allowing the parameters of a frequent word in the labeled training data to drift away from related but rare words in the embedding space.Previous work partially addresses these problems by estimating concept embeddings in WordNet (e.g., Rothe and Schütze, 2015), or improving word representations using information from knowledge graphs (e.g., Faruqui et al., 2015). However, it is still not clear how to use a lexical ontology to derive context-sensitive token embeddings.In this work, we represent a word token in a given context by estimating a context-sensitive probability distribution over relevant concepts in WordNet (Miller, 1995) and use the expected value (i.e., weighted sum) of the concept embeddings as the token representation (see §2). We take a task-centric approach towards doing this, and learn the token representations jointly with the task-specific parameters. In addition to providing context-sensitive token embeddings, the proposed method implicitly regularizes the embeddings of related words by forcing related words to share similar concept embeddings. As a result, the representation of a rare word which does not appear in the training data for a downstream task benefits from all the updates to related words which share one or more concept embeddings. Figure 1: An example grounding for the word 'pool'. Solid arrows represent possible senses and dashed arrows represent hypernym relations. Note that the same set of concepts are used to ground the word 'pool' regardless of its context. Other WordNet senses for 'pool' were removed from the figure for simplicity.Our approach to context-sensitive embeddings assumes the availability of a lexical ontology. While this work relies on WordNet, and we exploit the order of senses given by WordNet, our model is, in principle applicable to any ontology, with appropriate modifications. In this work, we do not assume the inputs are sense tagged. We use the proposed embeddings to predict prepositional phrase (PP) attachments (see §3), a challenging problem which emphasizes the selectional preferences between words in the PP and each of the candidate head words. Our empirical results and detailed analysis (see §4) show that the proposed embeddings effectively use WordNet to improve the accuracy of PP attachment predictions.</introduction>
<file>P17-1191-Figure1-1.png</file>
<figure>2</figure>
<caption>Figure 2: Steps for computing the contextsensitive token embedding for the word ‘pool’, as described in §2.2.</caption>
<mention10>"... a specific instantiation of this function in §3. To summarize, Figure 2 illustrates how to compute the embedding of a word ..."</mention10>
<mention20>"... on the encoder used to encode the context. We describe a specific instantiation of this function in §3. To summarize, Figure 2 illustrates how to compute the embedding of a word token t i = 'pool' in a given context: 1. ..."</mention20>
<mention50>"... tmax layer. This component is inspired by the soft attention often used in neural machine translation ( Bahdanau et al., 2014). 3 The definition of the context function is dependent on the encoder used to encode the context. We describe a specific instantiation of this function in §3. To summarize, Figure 2 illustrates how to compute the embedding of a word token t i = 'pool' in a given context: 1. compute a summary of the context context(i, t), 2. enumerate related concepts for t i , 3. compute p(s, s | t, w, i) for each pair (s, s ), ..."</mention50>
<lines3> We describe a specific instantiation of this function in §3. To summarize, Figure 2 illustrates how to compute the embedding of a word token t i = 'pool' in a given context: 1. compute a summary of the context context(i, t), 2. enumerate related concepts for t i , 3. compute p(s, s | t, w, i) for each pair (s, s ), and 1n the following section, we describe our model for predicting PP attachments, including our definition for context. Disambiguating PP attachments is an important and challenging NLP problem"</lines3>
<lines5> 3 The definition of the context function is dependent on the encoder used to encode the context. We describe a specific instantiation of this function in §3. To summarize, Figure 2 illustrates how to compute the embedding of a word token t i = 'pool' in a given context: 1. compute a summary of the context context(i, t), 2. enumerate related concepts for t i , 3. compute p(s, s | t, w, i) for each pair (s, s ), and 1n the following section, we describe our model for predicting PP attachments, including our definition for context. Disambiguating PP attachments is an important and challenging NLP problem. Since modeling hypernymy and selectional preferences is critical for successful prediction of PP attachments (Resnik, 1993), it is a good fit for evaluating our WordNet-grounded context-sensitive embeddings"</lines5>
<snippet3> We describe a specific instantiation of this function in §3. To summarize, Figure 2 illustrates how to compute the embedding of a word token t i = 'pool' in a given context: 1. compute a summary of the context context(i, t), 2. enumerate related concepts for t i , 3. compute p(s, s | t, w, i) for each pair (s, s ), and 1n the following section, we describe our model for predicting PP attachments, including our definition for context. Disambiguating PP attachments is an important and challenging NLP problem"</snippet3>
<snippet5> 3 The definition of the context function is dependent on the encoder used to encode the context. We describe a specific instantiation of this function in §3. To summarize, Figure 2 illustrates how to compute the embedding of a word token t i = 'pool' in a given context: 1. compute a summary of the context context(i, t), 2. enumerate related concepts for t i , 3. compute p(s, s | t, w, i) for each pair (s, s ), and 1n the following section, we describe our model for predicting PP attachments, including our definition for context. Disambiguating PP attachments is an important and challenging NLP problem. Since modeling hypernymy and selectional preferences is critical for successful prediction of PP attachments (Resnik, 1993), it is a good fit for evaluating our WordNet-grounded context-sensitive embeddings"</snippet5>
<abstract>Type-level word embeddings use the same set of parameters to represent all instances of a word regardless of its context, ignoring the inherent lexical ambiguity in language. Instead, we embed semantic concepts (or synsets) as defined in WordNet and represent a word token in a particular context by estimating a distribution over relevant semantic concepts. We use the new, context-sensitive embeddings in a model for predicting prepositional phrase (PP) attachments and jointly learn the concept embeddings and model parameters. We show that using context-sensitive em-beddings improves the accuracy of the PP attachment model by 5.4% absolute points, which amounts to a 34.4% relative reduction in errors.</abstract>
<title>Ontology-Aware Token Embeddings for Prepositional Phrase Attachment</title>
<introduction>Type-level word embeddings map a word type (i.e., a surface form) to a dense vector of real numbers such that similar word types have similar embeddings. When pre-trained on a large corpus of unlabeled text, they provide an effective mechanism for generalizing statistical models to words which do not appear in the labeled training data for a downstream task.In accordance with standard terminology, we make the following distinction between types and tokens in this paper: By word types, we mean the surface form of the word, whereas by tokens we mean the instantiation of the surface form in a context. For example, the same word type 'pool' occurs as two different tokens in the sentences "He sat by the pool," and "He played a game of pool."Most word embedding models define a single vector for each word type. However, a fundamental flaw in this design is their inability to distinguish between different meanings and abstractions of the same word. In the two sentences shown above, the word 'pool' has different meanings, but the same representation is typically used for both of them. Similarly, the fact that 'pool' and 'lake' are both kinds of water bodies is not explicitly incorporated in most type-level embeddings. Furthermore, it has become a standard practice to tune pre-trained word embeddings as model parameters during training for an NLP task (e.g., Chen and Manning, 2014;Lample et al., 2016), potentially allowing the parameters of a frequent word in the labeled training data to drift away from related but rare words in the embedding space.Previous work partially addresses these problems by estimating concept embeddings in WordNet (e.g., Rothe and Schütze, 2015), or improving word representations using information from knowledge graphs (e.g., Faruqui et al., 2015). However, it is still not clear how to use a lexical ontology to derive context-sensitive token embeddings.In this work, we represent a word token in a given context by estimating a context-sensitive probability distribution over relevant concepts in WordNet (Miller, 1995) and use the expected value (i.e., weighted sum) of the concept embeddings as the token representation (see §2). We take a task-centric approach towards doing this, and learn the token representations jointly with the task-specific parameters. In addition to providing context-sensitive token embeddings, the proposed method implicitly regularizes the embeddings of related words by forcing related words to share similar concept embeddings. As a result, the representation of a rare word which does not appear in the training data for a downstream task benefits from all the updates to related words which share one or more concept embeddings. Figure 1: An example grounding for the word 'pool'. Solid arrows represent possible senses and dashed arrows represent hypernym relations. Note that the same set of concepts are used to ground the word 'pool' regardless of its context. Other WordNet senses for 'pool' were removed from the figure for simplicity.Our approach to context-sensitive embeddings assumes the availability of a lexical ontology. While this work relies on WordNet, and we exploit the order of senses given by WordNet, our model is, in principle applicable to any ontology, with appropriate modifications. In this work, we do not assume the inputs are sense tagged. We use the proposed embeddings to predict prepositional phrase (PP) attachments (see §3), a challenging problem which emphasizes the selectional preferences between words in the PP and each of the candidate head words. Our empirical results and detailed analysis (see §4) show that the proposed embeddings effectively use WordNet to improve the accuracy of PP attachment predictions.</introduction>
<file>P17-1191-Figure2-1.png</file>
<figure>3</figure>
<caption>Figure 3: Two sentences illustrating the importance of lexicalization in PP attachment decisions. In the top sentence, the PP ‘with butter’ attaches to the noun ‘spaghetti’. In the bottom sentence, the PP ‘with chopsticks’ attaches to the verb ‘ate’. Note: This figure and caption have been reproduced from Belinkov et al. (2014).</caption>
<mention10>"... is a good fit for evaluating our WordNet-grounded context-sensitive embeddings. Figure 3, reproduced from Belinkov et al. (2014), illustrates an example ..."</mention10>
<mention20>"... critical for successful prediction of PP attachments (Resnik, 1993), it is a good fit for evaluating our WordNet-grounded context-sensitive embeddings. Figure 3, reproduced from Belinkov et al. (2014), illustrates an example of the PP attachment prediction problem. The accuracy of a ..."</mention20>
<mention50>"... we describe our model for predicting PP attachments, including our definition for context. Disambiguating PP attachments is an important and challenging NLP problem. Since modeling hypernymy and selectional preferences is critical for successful prediction of PP attachments (Resnik, 1993), it is a good fit for evaluating our WordNet-grounded context-sensitive embeddings. Figure 3, reproduced from Belinkov et al. (2014), illustrates an example of the PP attachment prediction problem. The accuracy of a competitive English dependency parser at predicting the head word of an ambiguous prepositional phrase is 88.5%, significantly lower than the overall unlabeled attachment accuracy of the same parser (94.2%). 4 ..."</mention50>
<lines3> Since modeling hypernymy and selectional preferences is critical for successful prediction of PP attachments (Resnik, 1993), it is a good fit for evaluating our WordNet-grounded context-sensitive embeddings. Figure 3, reproduced from Belinkov et al (2014), illustrates an example of the PP attachment prediction problem. The accuracy of a competitive English dependency parser at predicting the head word of an ambiguous prepositional phrase is 88.5%, significantly lower than the overall unlabeled attachment accuracy of the same parser (94.2%)"</lines3>
<lines5> Disambiguating PP attachments is an important and challenging NLP problem. Since modeling hypernymy and selectional preferences is critical for successful prediction of PP attachments (Resnik, 1993), it is a good fit for evaluating our WordNet-grounded context-sensitive embeddings. Figure 3, reproduced from Belinkov et al (2014), illustrates an example of the PP attachment prediction problem. The accuracy of a competitive English dependency parser at predicting the head word of an ambiguous prepositional phrase is 88.5%, significantly lower than the overall unlabeled attachment accuracy of the same parser (94.2%). 4 This section formally defines the problem of PP attachment disambiguation, describes our baseline model, then shows how to integrate the token-level embeddings in the model"</lines5>
<snippet3> Since modeling hypernymy and selectional preferences is critical for successful prediction of PP attachments (Resnik, 1993), it is a good fit for evaluating our WordNet-grounded context-sensitive embeddings. Figure 3, reproduced from Belinkov et al (2014), illustrates an example of the PP attachment prediction problem. The accuracy of a competitive English dependency parser at predicting the head word of an ambiguous prepositional phrase is 88.5%, significantly lower than the overall unlabeled attachment accuracy of the same parser (94.2%)"</snippet3>
<snippet5> Disambiguating PP attachments is an important and challenging NLP problem. Since modeling hypernymy and selectional preferences is critical for successful prediction of PP attachments (Resnik, 1993), it is a good fit for evaluating our WordNet-grounded context-sensitive embeddings. Figure 3, reproduced from Belinkov et al (2014), illustrates an example of the PP attachment prediction problem. The accuracy of a competitive English dependency parser at predicting the head word of an ambiguous prepositional phrase is 88.5%, significantly lower than the overall unlabeled attachment accuracy of the same parser (94.2%). 4 This section formally defines the problem of PP attachment disambiguation, describes our baseline model, then shows how to integrate the token-level embeddings in the model"</snippet5>
<abstract>Type-level word embeddings use the same set of parameters to represent all instances of a word regardless of its context, ignoring the inherent lexical ambiguity in language. Instead, we embed semantic concepts (or synsets) as defined in WordNet and represent a word token in a particular context by estimating a distribution over relevant semantic concepts. We use the new, context-sensitive embeddings in a model for predicting prepositional phrase (PP) attachments and jointly learn the concept embeddings and model parameters. We show that using context-sensitive em-beddings improves the accuracy of the PP attachment model by 5.4% absolute points, which amounts to a 34.4% relative reduction in errors.</abstract>
<title>Ontology-Aware Token Embeddings for Prepositional Phrase Attachment</title>
<introduction>Type-level word embeddings map a word type (i.e., a surface form) to a dense vector of real numbers such that similar word types have similar embeddings. When pre-trained on a large corpus of unlabeled text, they provide an effective mechanism for generalizing statistical models to words which do not appear in the labeled training data for a downstream task.In accordance with standard terminology, we make the following distinction between types and tokens in this paper: By word types, we mean the surface form of the word, whereas by tokens we mean the instantiation of the surface form in a context. For example, the same word type 'pool' occurs as two different tokens in the sentences "He sat by the pool," and "He played a game of pool."Most word embedding models define a single vector for each word type. However, a fundamental flaw in this design is their inability to distinguish between different meanings and abstractions of the same word. In the two sentences shown above, the word 'pool' has different meanings, but the same representation is typically used for both of them. Similarly, the fact that 'pool' and 'lake' are both kinds of water bodies is not explicitly incorporated in most type-level embeddings. Furthermore, it has become a standard practice to tune pre-trained word embeddings as model parameters during training for an NLP task (e.g., Chen and Manning, 2014;Lample et al., 2016), potentially allowing the parameters of a frequent word in the labeled training data to drift away from related but rare words in the embedding space.Previous work partially addresses these problems by estimating concept embeddings in WordNet (e.g., Rothe and Schütze, 2015), or improving word representations using information from knowledge graphs (e.g., Faruqui et al., 2015). However, it is still not clear how to use a lexical ontology to derive context-sensitive token embeddings.In this work, we represent a word token in a given context by estimating a context-sensitive probability distribution over relevant concepts in WordNet (Miller, 1995) and use the expected value (i.e., weighted sum) of the concept embeddings as the token representation (see §2). We take a task-centric approach towards doing this, and learn the token representations jointly with the task-specific parameters. In addition to providing context-sensitive token embeddings, the proposed method implicitly regularizes the embeddings of related words by forcing related words to share similar concept embeddings. As a result, the representation of a rare word which does not appear in the training data for a downstream task benefits from all the updates to related words which share one or more concept embeddings. Figure 1: An example grounding for the word 'pool'. Solid arrows represent possible senses and dashed arrows represent hypernym relations. Note that the same set of concepts are used to ground the word 'pool' regardless of its context. Other WordNet senses for 'pool' were removed from the figure for simplicity.Our approach to context-sensitive embeddings assumes the availability of a lexical ontology. While this work relies on WordNet, and we exploit the order of senses given by WordNet, our model is, in principle applicable to any ontology, with appropriate modifications. In this work, we do not assume the inputs are sense tagged. We use the proposed embeddings to predict prepositional phrase (PP) attachments (see §3), a challenging problem which emphasizes the selectional preferences between words in the PP and each of the candidate head words. Our empirical results and detailed analysis (see §4) show that the proposed embeddings effectively use WordNet to improve the accuracy of PP attachment predictions.</introduction>
<file>P17-1191-Figure3-1.png</file>
<figure>4</figure>
<caption>Figure 4: Effect of training data size on test accuracies of OntoLSTM-PP and LSTM-PP.</caption>
<mention10>"... accuracies on the test set. The plot is shown in Figure 4. As expected, the gap tends to be larger at ..."</mention10>
<mention20>"... models with different amounts of training data and measured their accuracies on the test set. The plot is shown in Figure 4. As expected, the gap tends to be larger at smaller data sizes. Surprisingly, even with 2000 sentences in the ..."</mention20>
<mention50>"... external information, the gap between the model and LSTM-PP is expected to be more pronounced when the training data sizes are smaller. To test this hypothesis, we trained the two models with different amounts of training data and measured their accuracies on the test set. The plot is shown in Figure 4. As expected, the gap tends to be larger at smaller data sizes. Surprisingly, even with 2000 sentences in the training data set, OntoLSTM-PP outperforms LSTM-PP trained with the full data set. When both the models are trained with the full dataset, LSTM-PP reaches a training accuracy of 95.3%, whereas ..."</mention50>
<lines3> To test this hypothesis, we trained the two models with different amounts of training data and measured their accuracies on the test set. The plot is shown in Figure 4. As expected, the gap tends to be larger at smaller data sizes"</lines3>
<lines5> Since OntoLSTM-PP uses external information, the gap between the model and LSTM-PP is expected to be more pronounced when the training data sizes are smaller. To test this hypothesis, we trained the two models with different amounts of training data and measured their accuracies on the test set. The plot is shown in Figure 4. As expected, the gap tends to be larger at smaller data sizes. Surprisingly, even with 2000 sentences in the training data set, OntoLSTM-PP outperforms LSTM-PP trained with the full data set"</lines5>
<snippet3> To test this hypothesis, we trained the two models with different amounts of training data and measured their accuracies on the test set. The plot is shown in Figure 4. As expected, the gap tends to be larger at smaller data sizes"</snippet3>
<snippet5> Since OntoLSTM-PP uses external information, the gap between the model and LSTM-PP is expected to be more pronounced when the training data sizes are smaller. To test this hypothesis, we trained the two models with different amounts of training data and measured their accuracies on the test set. The plot is shown in Figure 4. As expected, the gap tends to be larger at smaller data sizes. Surprisingly, even with 2000 sentences in the training data set, OntoLSTM-PP outperforms LSTM-PP trained with the full data set"</snippet5>
<abstract>Type-level word embeddings use the same set of parameters to represent all instances of a word regardless of its context, ignoring the inherent lexical ambiguity in language. Instead, we embed semantic concepts (or synsets) as defined in WordNet and represent a word token in a particular context by estimating a distribution over relevant semantic concepts. We use the new, context-sensitive embeddings in a model for predicting prepositional phrase (PP) attachments and jointly learn the concept embeddings and model parameters. We show that using context-sensitive em-beddings improves the accuracy of the PP attachment model by 5.4% absolute points, which amounts to a 34.4% relative reduction in errors.</abstract>
<title>Ontology-Aware Token Embeddings for Prepositional Phrase Attachment</title>
<introduction>Type-level word embeddings map a word type (i.e., a surface form) to a dense vector of real numbers such that similar word types have similar embeddings. When pre-trained on a large corpus of unlabeled text, they provide an effective mechanism for generalizing statistical models to words which do not appear in the labeled training data for a downstream task.In accordance with standard terminology, we make the following distinction between types and tokens in this paper: By word types, we mean the surface form of the word, whereas by tokens we mean the instantiation of the surface form in a context. For example, the same word type 'pool' occurs as two different tokens in the sentences "He sat by the pool," and "He played a game of pool."Most word embedding models define a single vector for each word type. However, a fundamental flaw in this design is their inability to distinguish between different meanings and abstractions of the same word. In the two sentences shown above, the word 'pool' has different meanings, but the same representation is typically used for both of them. Similarly, the fact that 'pool' and 'lake' are both kinds of water bodies is not explicitly incorporated in most type-level embeddings. Furthermore, it has become a standard practice to tune pre-trained word embeddings as model parameters during training for an NLP task (e.g., Chen and Manning, 2014;Lample et al., 2016), potentially allowing the parameters of a frequent word in the labeled training data to drift away from related but rare words in the embedding space.Previous work partially addresses these problems by estimating concept embeddings in WordNet (e.g., Rothe and Schütze, 2015), or improving word representations using information from knowledge graphs (e.g., Faruqui et al., 2015). However, it is still not clear how to use a lexical ontology to derive context-sensitive token embeddings.In this work, we represent a word token in a given context by estimating a context-sensitive probability distribution over relevant concepts in WordNet (Miller, 1995) and use the expected value (i.e., weighted sum) of the concept embeddings as the token representation (see §2). We take a task-centric approach towards doing this, and learn the token representations jointly with the task-specific parameters. In addition to providing context-sensitive token embeddings, the proposed method implicitly regularizes the embeddings of related words by forcing related words to share similar concept embeddings. As a result, the representation of a rare word which does not appear in the training data for a downstream task benefits from all the updates to related words which share one or more concept embeddings. Figure 1: An example grounding for the word 'pool'. Solid arrows represent possible senses and dashed arrows represent hypernym relations. Note that the same set of concepts are used to ground the word 'pool' regardless of its context. Other WordNet senses for 'pool' were removed from the figure for simplicity.Our approach to context-sensitive embeddings assumes the availability of a lexical ontology. While this work relies on WordNet, and we exploit the order of senses given by WordNet, our model is, in principle applicable to any ontology, with appropriate modifications. In this work, we do not assume the inputs are sense tagged. We use the proposed embeddings to predict prepositional phrase (PP) attachments (see §3), a challenging problem which emphasizes the selectional preferences between words in the PP and each of the candidate head words. Our empirical results and detailed analysis (see §4) show that the proposed embeddings effectively use WordNet to improve the accuracy of PP attachment predictions.</introduction>
<file>P17-1191-Figure4-1.png</file>
<figure>5</figure>
<caption>Figure 5: Two examples from the test set where OntoLSTM-PP predicts the head correctly and LSTM-PP does not, along with weights by OntoLSTM-PP to synsets that contribute to token representations of infrequent word types. The prepositions are shown in bold, LSTM-PP’s predictions in red and OntoLSTMPP’s predictions in green. Words that are not candidate heads or dependents are shown in brackets.</caption>
<mention10>"... sentences contained words not seen frequently in the training data. Figure 5 shows two such cases. In both cases, the weights ..."</mention10>
<mention20>"... not by LSTM-PP. A common pattern observed was that those sentences contained words not seen frequently in the training data. Figure 5 shows two such cases. In both cases, the weights assigned by OntoLSTM-PP to infrequent words are also shown. The ..."</mention20>
<mention50>"... Qualitative analysis. To better understand the effect of WordNet grounding, we took a sample of 100 sentences from the test set whose PP attachments were correctly predicted by OntoLSTM-PP but not by LSTM-PP. A common pattern observed was that those sentences contained words not seen frequently in the training data. Figure 5 shows two such cases. In both cases, the weights assigned by OntoLSTM-PP to infrequent words are also shown. The word types soapsuds and buoyancy do not occur in the training data, but OntoLSTM-PP was able to leverage the parameters learned for the synsets that contributed to their token representations. ..."</mention50>
<lines3> A common pattern observed was that those sentences contained words not seen frequently in the training data. Figure 5 shows two such cases. 1n both cases, the weights assigned by OntoLSTM-PP to infrequent words are also shown"</lines3>
<lines5> To better understand the effect of WordNet grounding, we took a sample of 100 sentences from the test set whose PP attachments were correctly predicted by OntoLSTM-PP but not by LSTM-PP. A common pattern observed was that those sentences contained words not seen frequently in the training data. Figure 5 shows two such cases. 1n both cases, the weights assigned by OntoLSTM-PP to infrequent words are also shown. The word types soapsuds and buoyancy do not occur in the training data, but OntoLSTM-PP was able to leverage the parameters learned for the synsets that contributed to their token representations"</lines5>
<snippet3> A common pattern observed was that those sentences contained words not seen frequently in the training data. Figure 5 shows two such cases. 1n both cases, the weights assigned by OntoLSTM-PP to infrequent words are also shown"</snippet3>
<snippet5> To better understand the effect of WordNet grounding, we took a sample of 100 sentences from the test set whose PP attachments were correctly predicted by OntoLSTM-PP but not by LSTM-PP. A common pattern observed was that those sentences contained words not seen frequently in the training data. Figure 5 shows two such cases. 1n both cases, the weights assigned by OntoLSTM-PP to infrequent words are also shown. The word types soapsuds and buoyancy do not occur in the training data, but OntoLSTM-PP was able to leverage the parameters learned for the synsets that contributed to their token representations"</snippet5>
<abstract>Type-level word embeddings use the same set of parameters to represent all instances of a word regardless of its context, ignoring the inherent lexical ambiguity in language. Instead, we embed semantic concepts (or synsets) as defined in WordNet and represent a word token in a particular context by estimating a distribution over relevant semantic concepts. We use the new, context-sensitive embeddings in a model for predicting prepositional phrase (PP) attachments and jointly learn the concept embeddings and model parameters. We show that using context-sensitive em-beddings improves the accuracy of the PP attachment model by 5.4% absolute points, which amounts to a 34.4% relative reduction in errors.</abstract>
<title>Ontology-Aware Token Embeddings for Prepositional Phrase Attachment</title>
<introduction>Type-level word embeddings map a word type (i.e., a surface form) to a dense vector of real numbers such that similar word types have similar embeddings. When pre-trained on a large corpus of unlabeled text, they provide an effective mechanism for generalizing statistical models to words which do not appear in the labeled training data for a downstream task.In accordance with standard terminology, we make the following distinction between types and tokens in this paper: By word types, we mean the surface form of the word, whereas by tokens we mean the instantiation of the surface form in a context. For example, the same word type 'pool' occurs as two different tokens in the sentences "He sat by the pool," and "He played a game of pool."Most word embedding models define a single vector for each word type. However, a fundamental flaw in this design is their inability to distinguish between different meanings and abstractions of the same word. In the two sentences shown above, the word 'pool' has different meanings, but the same representation is typically used for both of them. Similarly, the fact that 'pool' and 'lake' are both kinds of water bodies is not explicitly incorporated in most type-level embeddings. Furthermore, it has become a standard practice to tune pre-trained word embeddings as model parameters during training for an NLP task (e.g., Chen and Manning, 2014;Lample et al., 2016), potentially allowing the parameters of a frequent word in the labeled training data to drift away from related but rare words in the embedding space.Previous work partially addresses these problems by estimating concept embeddings in WordNet (e.g., Rothe and Schütze, 2015), or improving word representations using information from knowledge graphs (e.g., Faruqui et al., 2015). However, it is still not clear how to use a lexical ontology to derive context-sensitive token embeddings.In this work, we represent a word token in a given context by estimating a context-sensitive probability distribution over relevant concepts in WordNet (Miller, 1995) and use the expected value (i.e., weighted sum) of the concept embeddings as the token representation (see §2). We take a task-centric approach towards doing this, and learn the token representations jointly with the task-specific parameters. In addition to providing context-sensitive token embeddings, the proposed method implicitly regularizes the embeddings of related words by forcing related words to share similar concept embeddings. As a result, the representation of a rare word which does not appear in the training data for a downstream task benefits from all the updates to related words which share one or more concept embeddings. Figure 1: An example grounding for the word 'pool'. Solid arrows represent possible senses and dashed arrows represent hypernym relations. Note that the same set of concepts are used to ground the word 'pool' regardless of its context. Other WordNet senses for 'pool' were removed from the figure for simplicity.Our approach to context-sensitive embeddings assumes the availability of a lexical ontology. While this work relies on WordNet, and we exploit the order of senses given by WordNet, our model is, in principle applicable to any ontology, with appropriate modifications. In this work, we do not assume the inputs are sense tagged. We use the proposed embeddings to predict prepositional phrase (PP) attachments (see §3), a challenging problem which emphasizes the selectional preferences between words in the PP and each of the candidate head words. Our empirical results and detailed analysis (see §4) show that the proposed embeddings effectively use WordNet to improve the accuracy of PP attachment predictions.</introduction>
<file>P17-1191-Figure5-1.png</file>