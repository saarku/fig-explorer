<figure>1</figure>
<caption>Figure 1: Graph-to-string derivation.</caption>
<mention10>"... al., 2016;Song et al., 2016;Pourdamghani et al., 2016). want-01 ARG0 Figure 1: Graph-to-string derivation. Flanigan et al. (2016) transform a given ...  graph-to-tree errors by directly learning graph-to-string rules. As shown in Figure 1, we learn a synchronous node replacement grammar (NRG) from ..."</mention10>
<mention20>"... there has been little work on AMR-to-text generation (Flanigan et al., 2016;Song et al., 2016;Pourdamghani et al., 2016). want-01 ARG0 Figure 1: Graph-to-string derivation. Flanigan et al. (2016) transform a given AMR graph into a spanning tree, before translating it to ...  to leverage the advantages of hierarchical rules without suffering from graph-to-tree errors by directly learning graph-to-string rules. As shown in Figure 1, we learn a synchronous node replacement grammar (NRG) from a corpus of aligned AMR and sentence pairs. At test ..."</mention20>
<mention50>"... et al., 2014;Wang et al., 2015;Peng et al., 2015;Vanderwende et al., 2015;Pust et al., 2015;Artzi et al., 2015;Groschwitz et al., 2015;Goodman et al., 2016;Zhou et al., 2016;Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016;Song et al., 2016;Pourdamghani et al., 2016). want-01 ARG0 Figure 1: Graph-to-string derivation. Flanigan et al. (2016) transform a given AMR graph into a spanning tree, before translating it to a sentence using a tree-to-string transducer. Their method leverages existing machine translation techniques, capturing hierarchical correspondences between the spanning tree and the surface string. However, it suffers from error propagation ...  traveling salesman problem, using local features and a language model to rank candidate sentences. However, their method does not learn hierarchical structural correspondences between AMR graphs and strings. We propose to leverage the advantages of hierarchical rules without suffering from graph-to-tree errors by directly learning graph-to-string rules. As shown in Figure 1, we learn a synchronous node replacement grammar (NRG) from a corpus of aligned AMR and sentence pairs. At test time, we apply a graph transducer to collapse input go-01   boy   want-01  ARG1   ARG0   ARG0   go-01  #X2#  ..."</mention50>
<lines3> Despite much literature so far on text-to-AMR parsing ( Flanigan et al, 2014;Wang et al, 2015;Peng et al, 2015;Vanderwende et al, 2015;Pust et al, 2015;Artzi et al, 2015;Groschwitz et al, 2015;Goodman et al, 2016;Zhou et al, 2016;Peng et al, 2017), there has been little work on AMR-to-text generation (Flanigan et al, 2016;Song et al, 2016;Pourdamghani et al, 2016). want-01 ARG0 Figure 1: Graph-to-string derivation. Flanigan et al (2016) transform a given AMR graph into a spanning tree, before translating it to a sentence using a tree-to-string transducer. ...  We propose to leverage the advantages of hierarchical rules without suffering from graph-to-tree errors by directly learning graph-to-string rules. As shown in Figure 1, we learn a synchronous node replacement grammar (NRG) from a corpus of aligned AMR and sentence pairs. At test time, we apply a graph transducer to collapse input go-01   boy   want-01  ARG1   ARG0   ARG0   go-01  #X2#   ARG1   ARG0   (root)   go-01   #X3#   want-01  ARG1   ARG0   ARG0   #S#  #X1#   {#S#}  {#X1#}  {#X2# to go}  {#X3# wants to go}  {the boy wants to go}   (a)  AMR: String:  Table 1: Example rule set AMR graphs and generate output strings according to the learned grammar"</lines3>
<lines5> AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing ( Flanigan et al, 2014;Wang et al, 2015;Peng et al, 2015;Vanderwende et al, 2015;Pust et al, 2015;Artzi et al, 2015;Groschwitz et al, 2015;Goodman et al, 2016;Zhou et al, 2016;Peng et al, 2017), there has been little work on AMR-to-text generation (Flanigan et al, 2016;Song et al, 2016;Pourdamghani et al, 2016). want-01 ARG0 Figure 1: Graph-to-string derivation. Flanigan et al (2016) transform a given AMR graph into a spanning tree, before translating it to a sentence using a tree-to-string transducer. Their method leverages existing machine translation techniques, capturing hierarchical correspondences between the spanning tree and the surface string. ...  However, their method does not learn hierarchical structural correspondences between AMR graphs and strings. We propose to leverage the advantages of hierarchical rules without suffering from graph-to-tree errors by directly learning graph-to-string rules. As shown in Figure 1, we learn a synchronous node replacement grammar (NRG) from a corpus of aligned AMR and sentence pairs. At test time, we apply a graph transducer to collapse input go-01   boy   want-01  ARG1   ARG0   ARG0   go-01  #X2#   ARG1   ARG0   (root)   go-01   #X3#   want-01  ARG1   ARG0   ARG0   #S#  #X1#   {#S#}  {#X1#}  {#X2# to go}  {#X3# wants to go}  {the boy wants to go}   (a)  AMR: String:  Table 1: Example rule set AMR graphs and generate output strings according to the learned grammar. Our system makes use of a log-linear model with real-valued features, tuned using MERT (Och, 2003), and beam search decoding"</lines5>
<snippet3> Despite much literature so far on text-to-AMR parsing ( Flanigan et al, 2014;Wang et al, 2015;Peng et al, 2015;Vanderwende et al, 2015;Pust et al, 2015;Artzi et al, 2015;Groschwitz et al, 2015;Goodman et al, 2016;Zhou et al, 2016;Peng et al, 2017), there has been little work on AMR-to-text generation (Flanigan et al, 2016;Song et al, 2016;Pourdamghani et al, 2016). want-01 ARG0 Figure 1: Graph-to-string derivation. Flanigan et al (2016) transform a given AMR graph into a spanning tree, before translating it to a sentence using a tree-to-string transducer.</snippet3>
<snippet5> AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing ( Flanigan et al, 2014;Wang et al, 2015;Peng et al, 2015;Vanderwende et al, 2015;Pust et al, 2015;Artzi et al, 2015;Groschwitz et al, 2015;Goodman et al, 2016;Zhou et al, 2016;Peng et al, 2017), there has been little work on AMR-to-text generation (Flanigan et al, 2016;Song et al, 2016;Pourdamghani et al, 2016). want-01 ARG0 Figure 1: Graph-to-string derivation. Flanigan et al (2016) transform a given AMR graph into a spanning tree, before translating it to a sentence using a tree-to-string transducer. Their method leverages existing machine translation techniques, capturing hierarchical correspondences between the spanning tree and the surface string.</snippet5>
<abstract>This paper addresses the task of AMR-to-text generation by leveraging synchronous node replacement grammar. During training , graph-to-string rules are learned using a heuristic extraction algorithm. At test time, a graph transducer is applied to collapse input AMRs and generate output sentences. Evaluated on a standard benchmark , our method gives the state-of-the-art result.</abstract>
<title>AMR-to-text Generation with Synchronous Node Replacement Grammar</title>
<introduction>Abstract Meaning Representation (AMR) ( Ba- narescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph. AMR uses a graph to represent meaning, where nodes (such as "boy", "want-01") represent concepts, and edges (such as "ARG0", "ARG1") represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012;Tamchyna et al., 2015), question answering ( Mitra and Baral, 2015), summarization ( Takase et al., 2016) and event detection ( Li et al., 2015).AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing ( Flanigan et al., 2014;Wang et al., 2015;Peng et al., 2015;Vanderwende et al., 2015;Pust et al., 2015;Artzi et al., 2015;Groschwitz et al., 2015;Goodman et al., 2016;Zhou et al., 2016;Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016;Song et al., 2016;Pourdamghani et al., 2016). want-01 ARG0</introduction>
<file>P17-2002-Figure1-1.png</file>
<figure>2</figure>
<caption>Figure 2: Example deduction procedure</caption>
<mention10>"... to S, and use subscripts to distinguish different non-terminal instances. Figure 2 shows an example derivation process for the sentence "the ..."</mention10>
<mention20>"...  (2005), we use only one nonterminal X in addition to S, and use subscripts to distinguish different non-terminal instances. Figure 2 shows an example derivation process for the sentence "the boy wants to go" given the rule set in Table ..."</mention20>
<mention50>"... F is connected to the rest of the graph when replacing X i with F on the graph. Here we omit defining C and allow arbitrary connections. 1 Following Chiang  (2005), we use only one nonterminal X in addition to S, and use subscripts to distinguish different non-terminal instances. Figure 2 shows an example derivation process for the sentence "the boy wants to go" given the rule set in Table 1. Given the start symbol S, which is first replaced with X 1 , rule (c) is applied to generate "X 2 to go" and its AMR counterpart. Then rule ..."</mention50>
<lines3> 1 Following Chiang  (2005), we use only one nonterminal X in addition to S, and use subscripts to distinguish different non-terminal instances. Figure 2 shows an example derivation process for the sentence "the boy wants to go" given the rule set in Table 1. Given the start symbol S, which is first replaced with X 1 , rule (c) is applied to generate "X 2 to go" and its AMR counterpart"</lines3>
<lines5> Here we omit defining C and allow arbitrary connections. 1 Following Chiang  (2005), we use only one nonterminal X in addition to S, and use subscripts to distinguish different non-terminal instances. Figure 2 shows an example derivation process for the sentence "the boy wants to go" given the rule set in Table 1. Given the start symbol S, which is first replaced with X 1 , rule (c) is applied to generate "X 2 to go" and its AMR counterpart. Then rule (b) is used to generate "X 3 wants" and its AMR counterpart from X 2"</lines5>
<snippet3> 1 Following Chiang  (2005), we use only one nonterminal X in addition to S, and use subscripts to distinguish different non-terminal instances. Figure 2 shows an example derivation process for the sentence "the boy wants to go" given the rule set in Table 1. Given the start symbol S, which is first replaced with X 1 , rule (c) is applied to generate "X 2 to go" and its AMR counterpart"</snippet3>
<snippet5> Here we omit defining C and allow arbitrary connections. 1 Following Chiang  (2005), we use only one nonterminal X in addition to S, and use subscripts to distinguish different non-terminal instances. Figure 2 shows an example derivation process for the sentence "the boy wants to go" given the rule set in Table 1. Given the start symbol S, which is first replaced with X 1 , rule (c) is applied to generate "X 2 to go" and its AMR counterpart. Then rule (b) is used to generate "X 3 wants" and its AMR counterpart from X 2"</snippet5>
<abstract>This paper addresses the task of AMR-to-text generation by leveraging synchronous node replacement grammar. During training , graph-to-string rules are learned using a heuristic extraction algorithm. At test time, a graph transducer is applied to collapse input AMRs and generate output sentences. Evaluated on a standard benchmark , our method gives the state-of-the-art result.</abstract>
<title>AMR-to-text Generation with Synchronous Node Replacement Grammar</title>
<introduction>Abstract Meaning Representation (AMR) ( Ba- narescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph. AMR uses a graph to represent meaning, where nodes (such as "boy", "want-01") represent concepts, and edges (such as "ARG0", "ARG1") represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012;Tamchyna et al., 2015), question answering ( Mitra and Baral, 2015), summarization ( Takase et al., 2016) and event detection ( Li et al., 2015).AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing ( Flanigan et al., 2014;Wang et al., 2015;Peng et al., 2015;Vanderwende et al., 2015;Pust et al., 2015;Artzi et al., 2015;Groschwitz et al., 2015;Goodman et al., 2016;Zhou et al., 2016;Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016;Song et al., 2016;Pourdamghani et al., 2016). want-01 ARG0</introduction>
<file>P17-2002-Figure2-1.png</file>
<figure>3</figure>
<caption>Figure 3: Statistics on the right-hand side.</caption>
<mention10>"..."</mention10>
<mention20>"..."</mention20>
<mention50>"..."</mention50>
<lines3>"</lines3>
<lines5>"</lines5>
<snippet3>"</snippet3>
<snippet5>"</snippet5>
<abstract>This paper addresses the task of AMR-to-text generation by leveraging synchronous node replacement grammar. During training , graph-to-string rules are learned using a heuristic extraction algorithm. At test time, a graph transducer is applied to collapse input AMRs and generate output sentences. Evaluated on a standard benchmark , our method gives the state-of-the-art result.</abstract>
<title>AMR-to-text Generation with Synchronous Node Replacement Grammar</title>
<introduction>Abstract Meaning Representation (AMR) ( Ba- narescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph. AMR uses a graph to represent meaning, where nodes (such as "boy", "want-01") represent concepts, and edges (such as "ARG0", "ARG1") represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012;Tamchyna et al., 2015), question answering ( Mitra and Baral, 2015), summarization ( Takase et al., 2016) and event detection ( Li et al., 2015).AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing ( Flanigan et al., 2014;Wang et al., 2015;Peng et al., 2015;Vanderwende et al., 2015;Pust et al., 2015;Artzi et al., 2015;Groschwitz et al., 2015;Goodman et al., 2016;Zhou et al., 2016;Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016;Song et al., 2016;Pourdamghani et al., 2016). want-01 ARG0</introduction>
<file>P17-2002-Figure3-1.png</file>