<figure>1</figure>
<caption>Figure 1: An example of the sentences with entity attributes annotated in TimeBank.</caption>
<mention10>"... addition, most of the systems include the entity attributes ( Figure 1) specified in TimeML 2 as basic features, which actually ..."</mention10>
<mention20>"... incorporate semantic relations between verbs from VerbOcean as features. In addition, most of the systems include the entity attributes ( Figure 1) specified in TimeML 2 as basic features, which actually need heavy human annotations. In this work, we push this ..."</mention20>
<mention50>"... classifiers exploit a variety of features. Laokulrat et al. (2013); Chambers et al. (2014) extract lexical and morphological features derived from WordNet synsets. Mani et al. (2006);D'Souza and Ng (2013) incorporate semantic relations between verbs from VerbOcean as features. In addition, most of the systems include the entity attributes ( Figure 1) specified in TimeML 2 as basic features, which actually need heavy human annotations. In this work, we push this work into a more practical level by using only word, part-of-speech (POS), dependency parsing information, without incorporating entity attributes, as well as any other external resources. In relation extraction, Bunescu ..."</mention50>
<lines3> Mani et al (2006);D'Souza and Ng (2013) incorporate semantic relations between verbs from VerbOcean as features. 1n addition, most of the systems include the entity attributes ( Figure 1) specified in TimeML 2 as basic features, which actually need heavy human annotations. 1n this work, we push this work into a more practical level by using only word, part-of-speech (POS), dependency parsing information, without incorporating entity attributes, as well as any other external resources"</lines3>
<lines5> Laokulrat et al (2013); Chambers et al (2014) extract lexical and morphological features derived from WordNet synsets. Mani et al (2006);D'Souza and Ng (2013) incorporate semantic relations between verbs from VerbOcean as features. 1n addition, most of the systems include the entity attributes ( Figure 1) specified in TimeML 2 as basic features, which actually need heavy human annotations. 1n this work, we push this work into a more practical level by using only word, part-of-speech (POS), dependency parsing information, without incorporating entity attributes, as well as any other external resources. 1n relation extraction, Bunescu and Mooney (2005) propose an observation that a relation can be captured by the shortest dependency path Our system is similar to the work by Xu et al (2015b)"</lines5>
<snippet3> Mani et al (2006);D'Souza and Ng (2013) incorporate semantic relations between verbs from VerbOcean as features. 1n addition, most of the systems include the entity attributes ( Figure 1) specified in TimeML 2 as basic features, which actually need heavy human annotations. 1n this work, we push this work into a more practical level by using only word, part-of-speech (POS), dependency parsing information, without incorporating entity attributes, as well as any other external resources"</snippet3>
<snippet5> Laokulrat et al (2013); Chambers et al (2014) extract lexical and morphological features derived from WordNet synsets. Mani et al (2006);D'Souza and Ng (2013) incorporate semantic relations between verbs from VerbOcean as features. 1n addition, most of the systems include the entity attributes ( Figure 1) specified in TimeML 2 as basic features, which actually need heavy human annotations. 1n this work, we push this work into a more practical level by using only word, part-of-speech (POS), dependency parsing information, without incorporating entity attributes, as well as any other external resources. 1n relation extraction, Bunescu and Mooney (2005) propose an observation that a relation can be captured by the shortest dependency path Our system is similar to the work by Xu et al (2015b)"</snippet5>
<abstract>Temporal relation classification is becoming an active research field. Lots of methods have been proposed, while most of them focus on extracting features from external resources. Less attention has been paid to a significant advance in a closely related task: relation extraction. In this work, we borrow a state-of-the-art method in relation extraction by adopting bidirectional long short-term memory (Bi-LSTM) along dependency paths (DP). We make a &quot;common root&quot; assumption to extend DP representations of cross-sentence links. In the final comparison to two state-of-the-art systems on TimeBank-Dense, our model achieves comparable performance , without using external knowledge and manually annotated attributes of entities (class, tense, polarity, etc.).</abstract>
<title>Classifying Temporal Relations by Bidirectional LSTM over Dependency Paths</title>
<introduction>Recently, the need for extracting temporal information from text is motivated rapidly by many NLP tasks such as: question answering (QA), information extraction (IE), etc. Along with the TimeBank 1 ( Pustejovsky et al., 2003) and other temporal information annotated corpora, a series of temporal evaluation challenges (TempEval-1,2,3) ( Verhagen et al., 2009Verhagen et al., , 2010UzZaman et al., 2012) are attracting growing research efforts.Temporal relation classification is a task to identify the pairs of temporal entities (events or temporal expressions) that have a temporal link and classify the temporal relations between them. For instance, we show an event-event (E-E) link with 'DURING' type in (i), an event-time (E-T) link with 'INCLUDES' type in (ii) and an event-DCT (document creation time, E-D) with 'BEFORE' type in (iii).(i) There was no hint of trouble in the last conversation between controllers and TWA pilot Steven Snyder.(ii) In Washington today, the Federal Aviation Administration released air traffic control tapes.(iii) The U.S. Navy has 27 ships in the maritime barricade of Iraq.Marcu and Echihabi (2002) propose an approach considering word-based pairs as useful features. The following researchers (Laokulrat et al., 2013;Chambers et al., 2014;Mani et al., 2006;D'Souza and Ng, 2013) focus on extracting lexical, syntactic or semantic information from various external knowledge bases such as: WordNet (Miller, 1995) and VerbOcean ( Chklovski and Pantel, 2004). However, these feature based methods rely on hand-crafted efforts and external resources. In addition, these works require the features of entity attributes (class, tense, polarity, etc.), which are manually annotated to achieve high performance. Consequently, they are hard to obtain in practical application scenarios.In relation extraction, there is an explosion of the works done with the dependency path (DP) based methods, which employ various models along dependency paths ( Bunescu and Mooney, 2005;Plank and Moschitti, 2013). In recent years, the DP-based neural networks (Socher et al., 2011;Xu et al., 2015a,b) show state-of-the-art performance, with less requirements on explicit features. Intuitively, the DP-based approaches have the potential to classify temporal relations.Both relation extraction and temporal relation classification require the identification of relation- ship between entities in texts. However, temporal relation classification is more challenging, since it includes three different type of entities: 'event', 'time expression' and DCT. Cross-sentence links also add additional complexity into the task. Due to the outstanding performance of DP-based neural networks revealed in relation extraction, we borrow this state-of-the-art approach to temporal relation classification.In Section 2 of this paper, we review related work and introduce TimeBank-Dense. We discuss the cross-sentence link problem and the architectures of our E-E, E-T and E-D classifiers in Section 3. In Section 4, the experiments are performed on TimeBank-Dense and we compare our model to the baseline and two state-of-the-art systems. The final conclusion is made in Section 5.</introduction>
<file>P17-2001-Figure1-1.png</file>
<figure>2</figure>
<caption>Figure 2: An example of the DP representation of a cross-sentence link between the two sentences in Figure 1.</caption>
<mention10>"... from the ends to the "common root", as shown in Figure 2. Stanford CoreNLP 4 is used to parsing syntactic structures ..."</mention10>
<mention20>"... path can be represented as two shortest dependency path branches from the ends to the "common root", as shown in Figure 2. Stanford CoreNLP 4 is used to parsing syntactic structures of sentences in this work. Long short-term memory (LSTM) (Hochreiter ..."</mention20>
<mention50>"... how to represent the dependency path of a cross-sentence link. In this work, we make a naive assumption that two neighboring sentences share a "common root". Therefore, a cross-sentence dependency path can be represented as two shortest dependency path branches from the ends to the "common root", as shown in Figure 2. Stanford CoreNLP 4 is used to parsing syntactic structures of sentences in this work. Long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) is a natural choice for processing sequential dependency paths. As the reversed order also takes useful information, a backward representation can be achieved by feeding LSTM with ..."</mention50>
<lines3> 1n this work, we make a naive assumption that two neighboring sentences share a "common root". Therefore, a cross-sentence dependency path can be represented as two shortest dependency path branches from the ends to the "common root", as shown in Figure 2. Stanford CoreNLP 4 is used to parsing syntactic structures of sentences in this work"</lines3>
<lines5> A crucial obstacle is how to represent the dependency path of a cross-sentence link. 1n this work, we make a naive assumption that two neighboring sentences share a "common root". Therefore, a cross-sentence dependency path can be represented as two shortest dependency path branches from the ends to the "common root", as shown in Figure 2. Stanford CoreNLP 4 is used to parsing syntactic structures of sentences in this work. Long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) is a natural choice for processing sequential dependency paths"</lines5>
<snippet3> 1n this work, we make a naive assumption that two neighboring sentences share a "common root". Therefore, a cross-sentence dependency path can be represented as two shortest dependency path branches from the ends to the "common root", as shown in Figure 2. Stanford CoreNLP 4 is used to parsing syntactic structures of sentences in this work"</snippet3>
<snippet5> A crucial obstacle is how to represent the dependency path of a cross-sentence link. 1n this work, we make a naive assumption that two neighboring sentences share a "common root". Therefore, a cross-sentence dependency path can be represented as two shortest dependency path branches from the ends to the "common root", as shown in Figure 2. Stanford CoreNLP 4 is used to parsing syntactic structures of sentences in this work. Long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) is a natural choice for processing sequential dependency paths"</snippet5>
<abstract>Temporal relation classification is becoming an active research field. Lots of methods have been proposed, while most of them focus on extracting features from external resources. Less attention has been paid to a significant advance in a closely related task: relation extraction. In this work, we borrow a state-of-the-art method in relation extraction by adopting bidirectional long short-term memory (Bi-LSTM) along dependency paths (DP). We make a &quot;common root&quot; assumption to extend DP representations of cross-sentence links. In the final comparison to two state-of-the-art systems on TimeBank-Dense, our model achieves comparable performance , without using external knowledge and manually annotated attributes of entities (class, tense, polarity, etc.).</abstract>
<title>Classifying Temporal Relations by Bidirectional LSTM over Dependency Paths</title>
<introduction>Recently, the need for extracting temporal information from text is motivated rapidly by many NLP tasks such as: question answering (QA), information extraction (IE), etc. Along with the TimeBank 1 ( Pustejovsky et al., 2003) and other temporal information annotated corpora, a series of temporal evaluation challenges (TempEval-1,2,3) ( Verhagen et al., 2009Verhagen et al., , 2010UzZaman et al., 2012) are attracting growing research efforts.Temporal relation classification is a task to identify the pairs of temporal entities (events or temporal expressions) that have a temporal link and classify the temporal relations between them. For instance, we show an event-event (E-E) link with 'DURING' type in (i), an event-time (E-T) link with 'INCLUDES' type in (ii) and an event-DCT (document creation time, E-D) with 'BEFORE' type in (iii).(i) There was no hint of trouble in the last conversation between controllers and TWA pilot Steven Snyder.(ii) In Washington today, the Federal Aviation Administration released air traffic control tapes.(iii) The U.S. Navy has 27 ships in the maritime barricade of Iraq.Marcu and Echihabi (2002) propose an approach considering word-based pairs as useful features. The following researchers (Laokulrat et al., 2013;Chambers et al., 2014;Mani et al., 2006;D'Souza and Ng, 2013) focus on extracting lexical, syntactic or semantic information from various external knowledge bases such as: WordNet (Miller, 1995) and VerbOcean ( Chklovski and Pantel, 2004). However, these feature based methods rely on hand-crafted efforts and external resources. In addition, these works require the features of entity attributes (class, tense, polarity, etc.), which are manually annotated to achieve high performance. Consequently, they are hard to obtain in practical application scenarios.In relation extraction, there is an explosion of the works done with the dependency path (DP) based methods, which employ various models along dependency paths ( Bunescu and Mooney, 2005;Plank and Moschitti, 2013). In recent years, the DP-based neural networks (Socher et al., 2011;Xu et al., 2015a,b) show state-of-the-art performance, with less requirements on explicit features. Intuitively, the DP-based approaches have the potential to classify temporal relations.Both relation extraction and temporal relation classification require the identification of relation- ship between entities in texts. However, temporal relation classification is more challenging, since it includes three different type of entities: 'event', 'time expression' and DCT. Cross-sentence links also add additional complexity into the task. Due to the outstanding performance of DP-based neural networks revealed in relation extraction, we borrow this state-of-the-art approach to temporal relation classification.In Section 2 of this paper, we review related work and introduce TimeBank-Dense. We discuss the cross-sentence link problem and the architectures of our E-E, E-T and E-D classifiers in Section 3. In Section 4, the experiments are performed on TimeBank-Dense and we compare our model to the baseline and two state-of-the-art systems. The final conclusion is made in Section 5.</introduction>
<file>P17-2001-Figure2-1.png</file>
<figure>3</figure>
<caption>Figure 3: The DP-based Bi-LSTM temporal relation classifier.</caption>
<mention10>"... referred to as bidirectional LSTM ( Graves and Schmidhuber, 2005). Figure 3a shows the neural network architecture of our E-E, E-T ...  outputs fed into the penultimate hidden layer, as shown in Figure 3b. In this work, we use word2vec 5 The grid ..."</mention10>
<mention20>"... adopt the concatenation of the forward and backward LSTMs outputs, referred to as bidirectional LSTM ( Graves and Schmidhuber, 2005). Figure 3a shows the neural network architecture of our E-E, E-T classifier. Given an E-E or E-T temporal link, our system ...  applies a similar architecture, but with single branch Bi-LSTM with outputs fed into the penultimate hidden layer, as shown in Figure 3b. In this work, we use word2vec 5 The grid search exploring a full hyper-parameter space takes time for three ..."</mention20>
<mention50>"... choice for processing sequential dependency paths. As the reversed order also takes useful information, a backward representation can be achieved by feeding LSTM with the same input in reverse. We adopt the concatenation of the forward and backward LSTMs outputs, referred to as bidirectional LSTM ( Graves and Schmidhuber, 2005). Figure 3a shows the neural network architecture of our E-E, E-T classifier. Given an E-E or E-T temporal link, our system first generates two SDP branches: 1) the source entity to common root, 2) the target entity to common root. For each word along a SDP branch, concatenation of word, POS ...  are all concatenated, and fed into a fully connected hidden units layer. The final Softmax layer generates multi-class predictions. Since an E-D link contains single event SDP branch, our system applies a similar architecture, but with single branch Bi-LSTM with outputs fed into the penultimate hidden layer, as shown in Figure 3b. In this work, we use word2vec 5 The grid search exploring a full hyper-parameter space takes time for three classifiers (E-E, E-T and E-D). Empirically, we set each single LSTM output with the same dimensions (equal to 300) as the concatenation of word, POS, DEP embeddings. The hidden layer ..."</mention50>
<lines3> We adopt the concatenation of the forward and backward LSTMs outputs, referred to as bidirectional LSTM ( Graves and Schmidhuber, 2005). Figure 3a shows the neural network architecture of our E-E, E-T classifier. Given an E-E or E-T temporal link, our system first generates two SDP branches: 1) the source entity to common root, 2) the target entity to common root. ...  The final Softmax layer generates multi-class predictions. Since an E-D link contains single event SDP branch, our system applies a similar architecture, but with single branch Bi-LSTM with outputs fed into the penultimate hidden layer, as shown in Figure 3b. 1n this work, we use word2vec 5 The grid search exploring a full hyper-parameter space takes time for three classifiers (E-E, E-T and E-D)"</lines3>
<lines5> As the reversed order also takes useful information, a backward representation can be achieved by feeding LSTM with the same input in reverse. We adopt the concatenation of the forward and backward LSTMs outputs, referred to as bidirectional LSTM ( Graves and Schmidhuber, 2005). Figure 3a shows the neural network architecture of our E-E, E-T classifier. Given an E-E or E-T temporal link, our system first generates two SDP branches: 1) the source entity to common root, 2) the target entity to common root. For each word along a SDP branch, concatenation of word, POS and dependency relation (DEP) embeddings (word-level) is fed into Bi-LSTM. ...  The forward and backward outputs of both source and target branches are all concatenated, and fed into a fully connected hidden units layer. The final Softmax layer generates multi-class predictions. Since an E-D link contains single event SDP branch, our system applies a similar architecture, but with single branch Bi-LSTM with outputs fed into the penultimate hidden layer, as shown in Figure 3b. 1n this work, we use word2vec 5 The grid search exploring a full hyper-parameter space takes time for three classifiers (E-E, E-T and E-D). Empirically, we set each single LSTM output with the same dimensions (equal to 300) as the concatenation of word, POS, DEP embeddings"</lines5>
<snippet3> We adopt the concatenation of the forward and backward LSTMs outputs, referred to as bidirectional LSTM ( Graves and Schmidhuber, 2005). Figure 3a shows the neural network architecture of our E-E, E-T classifier. Given an E-E or E-T temporal link, our system first generates two SDP branches: 1) the source entity to common root, 2) the target entity to common root.</snippet3>
<snippet5> As the reversed order also takes useful information, a backward representation can be achieved by feeding LSTM with the same input in reverse. We adopt the concatenation of the forward and backward LSTMs outputs, referred to as bidirectional LSTM ( Graves and Schmidhuber, 2005). Figure 3a shows the neural network architecture of our E-E, E-T classifier. Given an E-E or E-T temporal link, our system first generates two SDP branches: 1) the source entity to common root, 2) the target entity to common root. For each word along a SDP branch, concatenation of word, POS and dependency relation (DEP) embeddings (word-level) is fed into Bi-LSTM.</snippet5>
<abstract>Temporal relation classification is becoming an active research field. Lots of methods have been proposed, while most of them focus on extracting features from external resources. Less attention has been paid to a significant advance in a closely related task: relation extraction. In this work, we borrow a state-of-the-art method in relation extraction by adopting bidirectional long short-term memory (Bi-LSTM) along dependency paths (DP). We make a &quot;common root&quot; assumption to extend DP representations of cross-sentence links. In the final comparison to two state-of-the-art systems on TimeBank-Dense, our model achieves comparable performance , without using external knowledge and manually annotated attributes of entities (class, tense, polarity, etc.).</abstract>
<title>Classifying Temporal Relations by Bidirectional LSTM over Dependency Paths</title>
<introduction>Recently, the need for extracting temporal information from text is motivated rapidly by many NLP tasks such as: question answering (QA), information extraction (IE), etc. Along with the TimeBank 1 ( Pustejovsky et al., 2003) and other temporal information annotated corpora, a series of temporal evaluation challenges (TempEval-1,2,3) ( Verhagen et al., 2009Verhagen et al., , 2010UzZaman et al., 2012) are attracting growing research efforts.Temporal relation classification is a task to identify the pairs of temporal entities (events or temporal expressions) that have a temporal link and classify the temporal relations between them. For instance, we show an event-event (E-E) link with 'DURING' type in (i), an event-time (E-T) link with 'INCLUDES' type in (ii) and an event-DCT (document creation time, E-D) with 'BEFORE' type in (iii).(i) There was no hint of trouble in the last conversation between controllers and TWA pilot Steven Snyder.(ii) In Washington today, the Federal Aviation Administration released air traffic control tapes.(iii) The U.S. Navy has 27 ships in the maritime barricade of Iraq.Marcu and Echihabi (2002) propose an approach considering word-based pairs as useful features. The following researchers (Laokulrat et al., 2013;Chambers et al., 2014;Mani et al., 2006;D'Souza and Ng, 2013) focus on extracting lexical, syntactic or semantic information from various external knowledge bases such as: WordNet (Miller, 1995) and VerbOcean ( Chklovski and Pantel, 2004). However, these feature based methods rely on hand-crafted efforts and external resources. In addition, these works require the features of entity attributes (class, tense, polarity, etc.), which are manually annotated to achieve high performance. Consequently, they are hard to obtain in practical application scenarios.In relation extraction, there is an explosion of the works done with the dependency path (DP) based methods, which employ various models along dependency paths ( Bunescu and Mooney, 2005;Plank and Moschitti, 2013). In recent years, the DP-based neural networks (Socher et al., 2011;Xu et al., 2015a,b) show state-of-the-art performance, with less requirements on explicit features. Intuitively, the DP-based approaches have the potential to classify temporal relations.Both relation extraction and temporal relation classification require the identification of relation- ship between entities in texts. However, temporal relation classification is more challenging, since it includes three different type of entities: 'event', 'time expression' and DCT. Cross-sentence links also add additional complexity into the task. Due to the outstanding performance of DP-based neural networks revealed in relation extraction, we borrow this state-of-the-art approach to temporal relation classification.In Section 2 of this paper, we review related work and introduce TimeBank-Dense. We discuss the cross-sentence link problem and the architectures of our E-E, E-T and E-D classifiers in Section 3. In Section 4, the experiments are performed on TimeBank-Dense and we compare our model to the baseline and two state-of-the-art systems. The final conclusion is made in Section 5.</introduction>
<file>P17-2001-Figure3-1.png</file>