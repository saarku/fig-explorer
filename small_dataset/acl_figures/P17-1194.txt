<figure>1</figure>
<caption>Figure 1: The unfolded network structure for a sequence labeling model with an additional language modeling objective, performing NER on the sentence ”Fischler proposes measures”. The input tokens are shown at the bottom, the expected output labels are at the top. Arrows above variables indicate the directionality of the component (forward or backward).</caption>
<mention10>"... language modeling objective in comparison to the sequence labeling objective. Figure 1 shows a diagram of the unfolded neural architecture, when ..."</mention10>
<mention20>"... parameter that is used to control the importance of the language modeling objective in comparison to the sequence labeling objective. Figure 1 shows a diagram of the unfolded neural architecture, when performing NER on a short sentence with 3 words. At ..."</mention20>
<mention50>"... objectives are combined with the training objective E from either Equation 6 or 7, resulting in a new cost function E for the sequence labeling model: where γ is a parameter that is used to control the importance of the language modeling objective in comparison to the sequence labeling objective. Figure 1 shows a diagram of the unfolded neural architecture, when performing NER on a short sentence with 3 words. At each token position, the network is optimised to predict the previous word, the current label, and the next word in the sequence. The added language modeling objective encourages the system ..."</mention50>
<lines3> These representations are then passed through softmax layers in order to predict the preceding and following word: The objective function for both components is then constructed as a regular language modeling objective, by calculating the negative loglikelihood of the next word in the sequence: Finally, these additional objectives are combined with the training objective E from either Equation 6 or 7, resulting in a new cost function E for the sequence labeling model: where γ is a parameter that is used to control the importance of the language modeling objective in comparison to the sequence labeling objective. Figure 1 shows a diagram of the unfolded neural architecture, when performing NER on a short sentence with 3 words. At each token position, the network is optimised to predict the previous word, the current label, and the next word in the sequence"</lines3>
<lines5> We also use the opportunity to map the representation to a smaller size -since language modeling is not the main goal, we restrict the number of parameters available for this component, forcing the model to generalise more using fewer resources. These representations are then passed through softmax layers in order to predict the preceding and following word: The objective function for both components is then constructed as a regular language modeling objective, by calculating the negative loglikelihood of the next word in the sequence: Finally, these additional objectives are combined with the training objective E from either Equation 6 or 7, resulting in a new cost function E for the sequence labeling model: where γ is a parameter that is used to control the importance of the language modeling objective in comparison to the sequence labeling objective. Figure 1 shows a diagram of the unfolded neural architecture, when performing NER on a short sentence with 3 words. At each token position, the network is optimised to predict the previous word, the current label, and the next word in the sequence. The added language modeling objective encourages the system to learn richer feature representations that are then reused for sequence labeling"</lines5>
<snippet3> These representations are then passed through softmax layers in order to predict the preceding and following word: The objective function for both components is then constructed as a regular language modeling objective, by calculating the negative loglikelihood of the next word in the sequence: Finally, these additional objectives are combined with the training objective E from either Equation 6 or 7, resulting in a new cost function E for the sequence labeling model: where γ is a parameter that is used to control the importance of the language modeling objective in comparison to the sequence labeling objective. Figure 1 shows a diagram of the unfolded neural architecture, when performing NER on a short sentence with 3 words. At each token position, the network is optimised to predict the previous word, the current label, and the next word in the sequence"</snippet3>
<snippet5> We also use the opportunity to map the representation to a smaller size -since language modeling is not the main goal, we restrict the number of parameters available for this component, forcing the model to generalise more using fewer resources. These representations are then passed through softmax layers in order to predict the preceding and following word: The objective function for both components is then constructed as a regular language modeling objective, by calculating the negative loglikelihood of the next word in the sequence: Finally, these additional objectives are combined with the training objective E from either Equation 6 or 7, resulting in a new cost function E for the sequence labeling model: where γ is a parameter that is used to control the importance of the language modeling objective in comparison to the sequence labeling objective. Figure 1 shows a diagram of the unfolded neural architecture, when performing NER on a short sentence with 3 words. At each token position, the network is optimised to predict the previous word, the current label, and the next word in the sequence. The added language modeling objective encourages the system to learn richer feature representations that are then reused for sequence labeling"</snippet5>
<abstract>We propose a sequence labeling framework with a secondary training objective , learning to predict surrounding words for every word in the dataset. This language modeling objective incentivises the system to learn general-purpose patterns of semantic and syntactic composition, which are also useful for improving accuracy on different sequence labeling tasks. The architecture was evaluated on a range of datasets, covering the tasks of error detection in learner texts, named entity recognition, chunking and POS-tagging. The novel language modeling objective provided consistent performance improvements on every benchmark, without requiring any additional annotated or unan-notated data.</abstract>
<title>Semi-supervised Multitask Learning for Sequence Labeling</title>
<introduction>Accurate and efficient sequence labeling models have a wide range of applications, including named entity recognition (NER), part-of-speech (POS) tagging, error detection and shallow parsing. Specialised approaches to sequence labeling often include extensive feature engineering, such as integrated gazetteers, capitalisation features, morphological information and POS tags. However, recent work has shown that neural network architectures are able to achieve comparable or improved performance, while automatically discovering useful features for a specific task and only requiring a sequence of tokens as input (Col- lobert et al., 2011;Irsoy and Cardie, 2014;Lample et al., 2016).This feature discovery is usually driven by an objective function based on predicting the annotated labels for each word, without much incentive to learn more general language features from the available text. In many sequence labeling tasks, the relevant labels in the dataset are very sparse and most of the words contribute very little to the training process. For example, in the CoNLL 2003 NER dataset (Tjong Kim Sang and De Meulder, 2003) only 17% of the tokens represent an entity. This ratio is even lower for error detection, with only 14% of all tokens being annotated as an error in the FCE dataset (Yannakoudakis et al., 2011). The sequence labeling models are able to learn this bias in the label distribution without obtaining much additional information from words that have the majority label (O for outside of an entity; C for correct word). Therefore, we propose an additional training objective which allows the models to make more extensive use of the available data.The task of language modeling offers an easily accessible objective -learning to predict the next word in the sequence requires only plain text as input, without relying on any particular annotation. Neural language modeling architectures also have many similarities to common sequence labeling frameworks: words are first mapped to distributed embeddings, followed by a recurrent neural network (RNN) module for composing word sequences into an informative context representation ( Mikolov et al., 2010;Graves et al., 2013;Chelba et al., 2013). Compared to any sequence labeling dataset, the task of language modeling has a considerably larger and more varied set of possible options to predict, making better use of each available word and encouraging the model to learn more general language features for accurate composition.In this paper, we propose a neural sequence labeling architecture that is also optimised as a language model, predicting surrounding words in the dataset in addition to assigning labels to each token. Specific sections of the network are op-timised as a forward-or backward-moving language model, while the label predictions are performed using context from both directions. This secondary unsupervised objective encourages the framework to learn richer features for semantic composition without requiring additional training data. We evaluate the sequence labeling model on 10 datasets from the fields of NER, POS-tagging, chunking and error detection in learner texts. Our experiments show that by including the unsupervised objective into the training process, the sequence labeling model achieves consistent performance improvements on all the benchmarks. This multitask training framework gives the largest improvements on error detection datasets, outperforming the previous state-of-the-art architecture.</introduction>
<file>P17-1194-Figure1-1.png</file>
<figure>2</figure>
<caption>Figure 2: F0.5 score on the FCE development set after each training epoch.</caption>
<mention10>"... also analysed the performance of the different architectures during training. Figure 2 shows the F 0.5 score on the development set ..."</mention10>
<mention20>"... corpus, the system can more easily detect any irregularities. We also analysed the performance of the different architectures during training. Figure 2 shows the F 0.5 score on the development set for each model after every epoch over the training data. ..."</mention20>
<mention50>"... a more varied training signal. 3. Finally, the task of error detection is directly related to language modeling. By learning a better model of the overall text in the training corpus, the system can more easily detect any irregularities. We also analysed the performance of the different architectures during training. Figure 2 shows the F 0.5 score on the development set for each model after every epoch over the training data. The baseline model peaks quickly, followed by a gradual drop in performance, which is likely due to overfitting on the available data. Dropout provides an effective regularisation method, slowing down ..."</mention50>
<lines3> We also analysed the performance of the different architectures during training. Figure 2 shows the F 0.5 score on the development set for each model after every epoch over the training data. The baseline model peaks quickly, followed by a gradual drop in performance, which is likely due to overfitting on the available data"</lines3>
<lines5> By learning a better model of the overall text in the training corpus, the system can more easily detect any irregularities. We also analysed the performance of the different architectures during training. Figure 2 shows the F 0.5 score on the development set for each model after every epoch over the training data. The baseline model peaks quickly, followed by a gradual drop in performance, which is likely due to overfitting on the available data. Dropout provides an effective regularisation method, slowing down the initial performance but preventing the model from overfitting"</lines5>
<snippet3> We also analysed the performance of the different architectures during training. Figure 2 shows the F 0.5 score on the development set for each model after every epoch over the training data. The baseline model peaks quickly, followed by a gradual drop in performance, which is likely due to overfitting on the available data"</snippet3>
<snippet5> By learning a better model of the overall text in the training corpus, the system can more easily detect any irregularities. We also analysed the performance of the different architectures during training. Figure 2 shows the F 0.5 score on the development set for each model after every epoch over the training data. The baseline model peaks quickly, followed by a gradual drop in performance, which is likely due to overfitting on the available data. Dropout provides an effective regularisation method, slowing down the initial performance but preventing the model from overfitting"</snippet5>
<abstract>We propose a sequence labeling framework with a secondary training objective , learning to predict surrounding words for every word in the dataset. This language modeling objective incentivises the system to learn general-purpose patterns of semantic and syntactic composition, which are also useful for improving accuracy on different sequence labeling tasks. The architecture was evaluated on a range of datasets, covering the tasks of error detection in learner texts, named entity recognition, chunking and POS-tagging. The novel language modeling objective provided consistent performance improvements on every benchmark, without requiring any additional annotated or unan-notated data.</abstract>
<title>Semi-supervised Multitask Learning for Sequence Labeling</title>
<introduction>Accurate and efficient sequence labeling models have a wide range of applications, including named entity recognition (NER), part-of-speech (POS) tagging, error detection and shallow parsing. Specialised approaches to sequence labeling often include extensive feature engineering, such as integrated gazetteers, capitalisation features, morphological information and POS tags. However, recent work has shown that neural network architectures are able to achieve comparable or improved performance, while automatically discovering useful features for a specific task and only requiring a sequence of tokens as input (Col- lobert et al., 2011;Irsoy and Cardie, 2014;Lample et al., 2016).This feature discovery is usually driven by an objective function based on predicting the annotated labels for each word, without much incentive to learn more general language features from the available text. In many sequence labeling tasks, the relevant labels in the dataset are very sparse and most of the words contribute very little to the training process. For example, in the CoNLL 2003 NER dataset (Tjong Kim Sang and De Meulder, 2003) only 17% of the tokens represent an entity. This ratio is even lower for error detection, with only 14% of all tokens being annotated as an error in the FCE dataset (Yannakoudakis et al., 2011). The sequence labeling models are able to learn this bias in the label distribution without obtaining much additional information from words that have the majority label (O for outside of an entity; C for correct word). Therefore, we propose an additional training objective which allows the models to make more extensive use of the available data.The task of language modeling offers an easily accessible objective -learning to predict the next word in the sequence requires only plain text as input, without relying on any particular annotation. Neural language modeling architectures also have many similarities to common sequence labeling frameworks: words are first mapped to distributed embeddings, followed by a recurrent neural network (RNN) module for composing word sequences into an informative context representation ( Mikolov et al., 2010;Graves et al., 2013;Chelba et al., 2013). Compared to any sequence labeling dataset, the task of language modeling has a considerably larger and more varied set of possible options to predict, making better use of each available word and encouraging the model to learn more general language features for accurate composition.In this paper, we propose a neural sequence labeling architecture that is also optimised as a language model, predicting surrounding words in the dataset in addition to assigning labels to each token. Specific sections of the network are op-timised as a forward-or backward-moving language model, while the label predictions are performed using context from both directions. This secondary unsupervised objective encourages the framework to learn richer features for semantic composition without requiring additional training data. We evaluate the sequence labeling model on 10 datasets from the fields of NER, POS-tagging, chunking and error detection in learner texts. Our experiments show that by including the unsupervised objective into the training process, the sequence labeling model achieves consistent performance improvements on all the benchmarks. This multitask training framework gives the largest improvements on error detection datasets, outperforming the previous state-of-the-art architecture.</introduction>
<file>P17-1194-Figure2-1.png</file>
<figure>3</figure>
<caption>Figure 3: Entity-level F1 score on the CHEMDNER development set after each training epoch.</caption>
<mention10>"... compared to 84.26% F 1 score on the CoNLL-03 dataset. Figure 3 shows F 1 on the CHEMDNER development set after ...  dropout allows the system to consistently outperform the other architectures. Figure 3: Entity-level F 1 score on the CHEMD-NER development set ..."</mention10>
<mention20>"... a similar architecture by Huang et al. (2015), achieving 86.26% compared to 84.26% F 1 score on the CoNLL-03 dataset. Figure 3 shows F 1 on the CHEMDNER development set after every training epoch. Without dropout, performance peaks quickly and then ...  improved. Finally, adding the language modeling objective on top of dropout allows the system to consistently outperform the other architectures. Figure 3: Entity-level F 1 score on the CHEMD-NER development set after each training epoch. We also evaluated the language modeling ..."</mention20>
<mention50>"... et al. (2016) achieve a considerably higher result of 90.94%, possibly due to their use of specialised word embeddings and a custom version of LSTM. However, our system does outperform a similar architecture by Huang et al. (2015), achieving 86.26% compared to 84.26% F 1 score on the CoNLL-03 dataset. Figure 3 shows F 1 on the CHEMDNER development set after every training epoch. Without dropout, performance peaks quickly and then trails off as the system overfits on the training set. Using dropout, the best performance is sustained throughout training and even slightly improved. Finally, adding the language modeling objective on top of dropout allows the system to consistently outperform the other architectures. Figure 3: Entity-level F 1 score on the CHEMD-NER development set after each training epoch. We also evaluated the language modeling training objective on four POS-tagging datasets. The Penn Treebank POS-tag corpus (Marcus et al., 1993) contains texts from the Wall Street Journal and has been annotated with 48 different part-of-speech ..."</mention50>
<lines3> However, our system does outperform a similar architecture by Huang et al (2015), achieving 86.26% compared to 84.26% F 1 score on the CoNLL-03 dataset. Figure 3 shows F 1 on the CHEMDNER development set after every training epoch. Without dropout, performance peaks quickly and then trails off as the system overfits on the training set. ...  Finally, adding the language modeling objective on top of dropout allows the system to consistently outperform the other architectures. Figure 3: Entity-level F 1 score on the CHEMD-NER development set after each training epoch. We also evaluated the language modeling training objective on four POS-tagging datasets"</lines3>
<lines5> On CoNLL-03, Lample et al (2016) achieve a considerably higher result of 90.94%, possibly due to their use of specialised word embeddings and a custom version of LSTM. However, our system does outperform a similar architecture by Huang et al (2015), achieving 86.26% compared to 84.26% F 1 score on the CoNLL-03 dataset. Figure 3 shows F 1 on the CHEMDNER development set after every training epoch. Without dropout, performance peaks quickly and then trails off as the system overfits on the training set. Using dropout, the best performance is sustained throughout training and even slightly improved. Finally, adding the language modeling objective on top of dropout allows the system to consistently outperform the other architectures. Figure 3: Entity-level F 1 score on the CHEMD-NER development set after each training epoch. We also evaluated the language modeling training objective on four POS-tagging datasets. The Penn Treebank POS-tag corpus (Marcus et al, 1993) contains texts from the Wall Street Journal and has been annotated with 48 different part-of-speech tags"</lines5>
<snippet3> However, our system does outperform a similar architecture by Huang et al (2015), achieving 86.26% compared to 84.26% F 1 score on the CoNLL-03 dataset. Figure 3 shows F 1 on the CHEMDNER development set after every training epoch. Without dropout, performance peaks quickly and then trails off as the system overfits on the training set.</snippet3>
<snippet5> On CoNLL-03, Lample et al (2016) achieve a considerably higher result of 90.94%, possibly due to their use of specialised word embeddings and a custom version of LSTM. However, our system does outperform a similar architecture by Huang et al (2015), achieving 86.26% compared to 84.26% F 1 score on the CoNLL-03 dataset. Figure 3 shows F 1 on the CHEMDNER development set after every training epoch. Without dropout, performance peaks quickly and then trails off as the system overfits on the training set. Using dropout, the best performance is sustained throughout training and even slightly improved. Finally, adding the language modeling objective on top of dropout allows the system to consistently outperform the other architectures. Figure 3: Entity-level F 1 score on the CHEMD-NER development set after each training epoch. We also evaluated the language modeling training objective on four POS-tagging datasets. The Penn Treebank POS-tag corpus (Marcus et al, 1993) contains texts from the Wall Street Journal and has been annotated with 48 different part-of-speech tags"</snippet5>
<abstract>We propose a sequence labeling framework with a secondary training objective , learning to predict surrounding words for every word in the dataset. This language modeling objective incentivises the system to learn general-purpose patterns of semantic and syntactic composition, which are also useful for improving accuracy on different sequence labeling tasks. The architecture was evaluated on a range of datasets, covering the tasks of error detection in learner texts, named entity recognition, chunking and POS-tagging. The novel language modeling objective provided consistent performance improvements on every benchmark, without requiring any additional annotated or unan-notated data.</abstract>
<title>Semi-supervised Multitask Learning for Sequence Labeling</title>
<introduction>Accurate and efficient sequence labeling models have a wide range of applications, including named entity recognition (NER), part-of-speech (POS) tagging, error detection and shallow parsing. Specialised approaches to sequence labeling often include extensive feature engineering, such as integrated gazetteers, capitalisation features, morphological information and POS tags. However, recent work has shown that neural network architectures are able to achieve comparable or improved performance, while automatically discovering useful features for a specific task and only requiring a sequence of tokens as input (Col- lobert et al., 2011;Irsoy and Cardie, 2014;Lample et al., 2016).This feature discovery is usually driven by an objective function based on predicting the annotated labels for each word, without much incentive to learn more general language features from the available text. In many sequence labeling tasks, the relevant labels in the dataset are very sparse and most of the words contribute very little to the training process. For example, in the CoNLL 2003 NER dataset (Tjong Kim Sang and De Meulder, 2003) only 17% of the tokens represent an entity. This ratio is even lower for error detection, with only 14% of all tokens being annotated as an error in the FCE dataset (Yannakoudakis et al., 2011). The sequence labeling models are able to learn this bias in the label distribution without obtaining much additional information from words that have the majority label (O for outside of an entity; C for correct word). Therefore, we propose an additional training objective which allows the models to make more extensive use of the available data.The task of language modeling offers an easily accessible objective -learning to predict the next word in the sequence requires only plain text as input, without relying on any particular annotation. Neural language modeling architectures also have many similarities to common sequence labeling frameworks: words are first mapped to distributed embeddings, followed by a recurrent neural network (RNN) module for composing word sequences into an informative context representation ( Mikolov et al., 2010;Graves et al., 2013;Chelba et al., 2013). Compared to any sequence labeling dataset, the task of language modeling has a considerably larger and more varied set of possible options to predict, making better use of each available word and encouraging the model to learn more general language features for accurate composition.In this paper, we propose a neural sequence labeling architecture that is also optimised as a language model, predicting surrounding words in the dataset in addition to assigning labels to each token. Specific sections of the network are op-timised as a forward-or backward-moving language model, while the label predictions are performed using context from both directions. This secondary unsupervised objective encourages the framework to learn richer features for semantic composition without requiring additional training data. We evaluate the sequence labeling model on 10 datasets from the fields of NER, POS-tagging, chunking and error detection in learner texts. Our experiments show that by including the unsupervised objective into the training process, the sequence labeling model achieves consistent performance improvements on all the benchmarks. This multitask training framework gives the largest improvements on error detection datasets, outperforming the previous state-of-the-art architecture.</introduction>
<file>P17-1194-Figure3-1.png</file>
<figure>4</figure>
<caption>Figure 4: Token-level accuracy on the PTB-POS development set after each training epoch.</caption>
<mention10>"... 3 and accuracy on the development set is shown in Figure 4. While the performance improvements are small, they are consistent ..."</mention10>
<mention20>"... sequence labeling architectures on POS-tagging can be seen in Table 3 and accuracy on the development set is shown in Figure 4. While the performance improvements are small, they are consistent across all domains, languages and datasets. Application of dropout again ..."</mention20>
<mention50>"... The baseline performance on these datasets is also close to the upper bound, therefore we expect the language modeling objective to not provide much additional benefit. The results of different sequence labeling architectures on POS-tagging can be seen in Table 3 and accuracy on the development set is shown in Figure 4. While the performance improvements are small, they are consistent across all domains, languages and datasets. Application of dropout again provides a more robust model, and the language modeling cost improves the performance further. Even though the labels already offer a varied training objective, learning to predict the surrounding words ..."</mention50>
<lines3> The baseline performance on these datasets is also close to the upper bound, therefore we expect the language modeling objective to not provide much additional benefit. The results of different sequence labeling architectures on POS-tagging can be seen in Table 3 and accuracy on the development set is shown in Figure 4. While the performance improvements are small, they are consistent across all domains, languages and datasets"</lines3>
<lines5> POS-tagging also offers a large variance of unique labels, with 48 labels in PTB and 42 in GEN1A, and this can provide useful information to the models during training. The baseline performance on these datasets is also close to the upper bound, therefore we expect the language modeling objective to not provide much additional benefit. The results of different sequence labeling architectures on POS-tagging can be seen in Table 3 and accuracy on the development set is shown in Figure 4. While the performance improvements are small, they are consistent across all domains, languages and datasets. Application of dropout again provides a more robust model, and the language modeling cost improves the performance further"</lines5>
<snippet3> The baseline performance on these datasets is also close to the upper bound, therefore we expect the language modeling objective to not provide much additional benefit. The results of different sequence labeling architectures on POS-tagging can be seen in Table 3 and accuracy on the development set is shown in Figure 4. While the performance improvements are small, they are consistent across all domains, languages and datasets"</snippet3>
<snippet5> POS-tagging also offers a large variance of unique labels, with 48 labels in PTB and 42 in GEN1A, and this can provide useful information to the models during training. The baseline performance on these datasets is also close to the upper bound, therefore we expect the language modeling objective to not provide much additional benefit. The results of different sequence labeling architectures on POS-tagging can be seen in Table 3 and accuracy on the development set is shown in Figure 4. While the performance improvements are small, they are consistent across all domains, languages and datasets. Application of dropout again provides a more robust model, and the language modeling cost improves the performance further"</snippet5>
<abstract>We propose a sequence labeling framework with a secondary training objective , learning to predict surrounding words for every word in the dataset. This language modeling objective incentivises the system to learn general-purpose patterns of semantic and syntactic composition, which are also useful for improving accuracy on different sequence labeling tasks. The architecture was evaluated on a range of datasets, covering the tasks of error detection in learner texts, named entity recognition, chunking and POS-tagging. The novel language modeling objective provided consistent performance improvements on every benchmark, without requiring any additional annotated or unan-notated data.</abstract>
<title>Semi-supervised Multitask Learning for Sequence Labeling</title>
<introduction>Accurate and efficient sequence labeling models have a wide range of applications, including named entity recognition (NER), part-of-speech (POS) tagging, error detection and shallow parsing. Specialised approaches to sequence labeling often include extensive feature engineering, such as integrated gazetteers, capitalisation features, morphological information and POS tags. However, recent work has shown that neural network architectures are able to achieve comparable or improved performance, while automatically discovering useful features for a specific task and only requiring a sequence of tokens as input (Col- lobert et al., 2011;Irsoy and Cardie, 2014;Lample et al., 2016).This feature discovery is usually driven by an objective function based on predicting the annotated labels for each word, without much incentive to learn more general language features from the available text. In many sequence labeling tasks, the relevant labels in the dataset are very sparse and most of the words contribute very little to the training process. For example, in the CoNLL 2003 NER dataset (Tjong Kim Sang and De Meulder, 2003) only 17% of the tokens represent an entity. This ratio is even lower for error detection, with only 14% of all tokens being annotated as an error in the FCE dataset (Yannakoudakis et al., 2011). The sequence labeling models are able to learn this bias in the label distribution without obtaining much additional information from words that have the majority label (O for outside of an entity; C for correct word). Therefore, we propose an additional training objective which allows the models to make more extensive use of the available data.The task of language modeling offers an easily accessible objective -learning to predict the next word in the sequence requires only plain text as input, without relying on any particular annotation. Neural language modeling architectures also have many similarities to common sequence labeling frameworks: words are first mapped to distributed embeddings, followed by a recurrent neural network (RNN) module for composing word sequences into an informative context representation ( Mikolov et al., 2010;Graves et al., 2013;Chelba et al., 2013). Compared to any sequence labeling dataset, the task of language modeling has a considerably larger and more varied set of possible options to predict, making better use of each available word and encouraging the model to learn more general language features for accurate composition.In this paper, we propose a neural sequence labeling architecture that is also optimised as a language model, predicting surrounding words in the dataset in addition to assigning labels to each token. Specific sections of the network are op-timised as a forward-or backward-moving language model, while the label predictions are performed using context from both directions. This secondary unsupervised objective encourages the framework to learn richer features for semantic composition without requiring additional training data. We evaluate the sequence labeling model on 10 datasets from the fields of NER, POS-tagging, chunking and error detection in learner texts. Our experiments show that by including the unsupervised objective into the training process, the sequence labeling model achieves consistent performance improvements on all the benchmarks. This multitask training framework gives the largest improvements on error detection datasets, outperforming the previous state-of-the-art architecture.</introduction>
<file>P17-1194-Figure4-1.png</file>