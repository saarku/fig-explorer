<figure>1</figure>
<caption>Figure 1: Computation Graph</caption>
<mention10>"... on the chosen meta-evaluation method. To tackle this prob-  Figure 1: Computation Graph lem we contribute a new objective function, ...  objective functions that are illustrated by the computation graph in Figure 1. Here we use the training objective very similar to ..."</mention10>
<mention20>"... that performance of these two objectives can vary radically depending on the chosen meta-evaluation method. To tackle this prob-  Figure 1: Computation Graph lem we contribute a new objective function, inspired by multi-task learning, in which we train for both ...  number of sentences in the corpus. Next we present several objective functions that are illustrated by the computation graph in Figure 1. Here we use the training objective very similar to BEER (Stanojevi´cStanojevi´c and Sima'an, 2014) which is a learning-to-rank framework ..."</mention20>
<mention50>"... level judgments into consideration. We first create a learning-to-rank model for ranking corpora and compare it to the standard learning-to-rank model that is trained for ranking sentences. This comparison shows that performance of these two objectives can vary radically depending on the chosen meta-evaluation method. To tackle this prob-  Figure 1: Computation Graph lem we contribute a new objective function, inspired by multi-task learning, in which we train for both objectives simultaneously. This multiobjective model behaves a lot more stable over all methods of meta-evaluation and achieves a higher correlation than both single objective models. All the models that we ...  because we want to get a score between 0 and 1: As the corpus level score we will use just the average of sentence level scores: where m is the number of sentences in the corpus. Next we present several objective functions that are illustrated by the computation graph in Figure 1. Here we use the training objective very similar to BEER (Stanojevi´cStanojevi´c and Sima'an, 2014) which is a learning-to-rank framework that finds a separating hyper-plane between "good" and "bad" translations. Unlike BEER, we use a max-margin objective instead of logistic regression. For each mini-batch we randomly select m human relative ..."</mention50>
<lines3> This comparison shows that performance of these two objectives can vary radically depending on the chosen meta-evaluation method. To tackle this prob-  Figure 1: Computation Graph lem we contribute a new objective function, inspired by multi-task learning, in which we train for both objectives simultaneously. This multiobjective model behaves a lot more stable over all methods of meta-evaluation and achieves a higher correlation than both single objective models. ...  By using broadcasting we can rewrite the previous definition of the f orward(·) function as: Now we can define the score of a sentence as a sigmoid function applied over the output of the f orward(·) function because we want to get a score between 0 and 1: As the corpus level score we will use just the average of sentence level scores: where m is the number of sentences in the corpus. Next we present several objective functions that are illustrated by the computation graph in Figure 1. Here we use the training objective very similar to BEER (Stanojevi´cStanojevi´c and Sima'an, 2014) which is a learning-to-rank framework that finds a separating hyper-plane between "good" and "bad" translations"</lines3>
<lines5> We first create a learning-to-rank model for ranking corpora and compare it to the standard learning-to-rank model that is trained for ranking sentences. This comparison shows that performance of these two objectives can vary radically depending on the chosen meta-evaluation method. To tackle this prob-  Figure 1: Computation Graph lem we contribute a new objective function, inspired by multi-task learning, in which we train for both objectives simultaneously. This multiobjective model behaves a lot more stable over all methods of meta-evaluation and achieves a higher correlation than both single objective models. All the models that we define have one basic function in common, we call it a f orward(·) function, that maps the features of any sentence to a single real number. ...  Usually in training we would like to process a mini-batch of feature vectors Φ, where Φ is a matrix in which each column is a feature vector of individual sentence in the mini-batch or in the corpus. By using broadcasting we can rewrite the previous definition of the f orward(·) function as: Now we can define the score of a sentence as a sigmoid function applied over the output of the f orward(·) function because we want to get a score between 0 and 1: As the corpus level score we will use just the average of sentence level scores: where m is the number of sentences in the corpus. Next we present several objective functions that are illustrated by the computation graph in Figure 1. Here we use the training objective very similar to BEER (Stanojevi´cStanojevi´c and Sima'an, 2014) which is a learning-to-rank framework that finds a separating hyper-plane between "good" and "bad" translations. Unlike BEER, we use a max-margin objective instead of logistic regression"</lines5>
<snippet3> This comparison shows that performance of these two objectives can vary radically depending on the chosen meta-evaluation method. To tackle this prob-  Figure 1: Computation Graph lem we contribute a new objective function, inspired by multi-task learning, in which we train for both objectives simultaneously. This multiobjective model behaves a lot more stable over all methods of meta-evaluation and achieves a higher correlation than both single objective models.</snippet3>
<snippet5> We first create a learning-to-rank model for ranking corpora and compare it to the standard learning-to-rank model that is trained for ranking sentences. This comparison shows that performance of these two objectives can vary radically depending on the chosen meta-evaluation method. To tackle this prob-  Figure 1: Computation Graph lem we contribute a new objective function, inspired by multi-task learning, in which we train for both objectives simultaneously. This multiobjective model behaves a lot more stable over all methods of meta-evaluation and achieves a higher correlation than both single objective models. All the models that we define have one basic function in common, we call it a f orward(·) function, that maps the features of any sentence to a single real number.</snippet5>
<abstract>MT evaluation metrics are tested for correlation with human judgments either at the sentence-or the corpus-level. Trained metrics ignore corpus-level judgments and are trained for high sentence-level correlation only. We show that training only for one objective (sentence or corpus level), can not only harm the performance on the other objective, but it can also be subopti-mal for the objective being optimized. To this end we present a metric trained for corpus-level and show empirical comparison against a metric trained for sentence-level exemplifying how their performance may vary per language pair, type and level of judgment. Subsequently we propose a model trained to optimize both objectives simultaneously and show that it is far more stable than-and on average outperforms-both models on both objectives.</abstract>
<title>Alternative Objective Functions for Training MT Evaluation Metrics</title>
<introduction>Ever since BLEU ( Papineni et al., 2002) many proposals for an improved automatic evaluation metric for Machine Translation (MT) have been made. Some proposals use additional information for extracting quality indicators, like paraphrasing ( Denkowski and Lavie, 2011), syntactic trees (Liu and Gildea, 2005;Stanojevi´cStanojevi´c and Sima'an, 2015) or shallow semantics ( Rios et al., 2011;Lo et al., 2012) etc. Whereas others use different matching strategies, like n-grams ( Papineni et al., 2002), treelets (Liu and Gildea, 2005) and skip-bigrams ( Lin and Och, 2004). Most metrics use several indicators of translation quality which are often combined in a linear model whose weights are estimated on a training set of human judgments.Because the most widely available type of human judgments are relative ranking (RR) judgments, the main machine learning method used for training the metrics were based on the learningto-rank framework (Li, 2011). While the effectiveness of this framework for training evaluation metrics has been confirmed many times, e.g., (Ye et al., 2007;Duh, 2008;Stanojevi´cStanojevi´c and Sima'an, 2014;Ma et al., 2016), so far there is no prior work exploring alternative objective functions for training learning-to-rank models. Without exception, all existing learning-to-rank models are trained to rank sentences while completely ignoring the corpora judgments, likely because human judgments come in the form of sentence rankings.It might seem that sentence and corpus level tasks are very similar but that is not the case. Empirically it has been shown that many metrics that perform well on the sentence level do not perform well on the corpus level and vice versa. By training to rank sentences the model does not necessarily learn to give scores that are well scaled, but only to give higher scores to better translations. Training for the corpus level score would force the metric to give well scaled scores on the sentence level.Human judgments of sentences can be aggregated in different ways to hypothesize human judgments of full corpora. However, this fact has not been used so far to train learning-to-rank models that are good for ranking different corpora.This work fills-in this gap by exploring the merits of different objective functions that take corpus level judgments into consideration. We first create a learning-to-rank model for ranking corpora and compare it to the standard learning-to-rank model that is trained for ranking sentences. This comparison shows that performance of these two objectives can vary radically depending on the chosen meta-evaluation method. To tackle this prob-  Figure 1: Computation Graph lem we contribute a new objective function, inspired by multi-task learning, in which we train for both objectives simultaneously. This multiobjective model behaves a lot more stable over all methods of meta-evaluation and achieves a higher correlation than both single objective models.</introduction>
<file>P17-2004-Figure1-1.png</file>