<figure>2</figure>
<caption>Figure 2: Distribution of AP over 100 class labels in WeightedSet. The proposed method (red) and the baseline method (blue) achieve high AP for the same number of classes, but ModsI additionally finds instances for classes for which the baseline returns nothing.</caption>
<mention10>"... shows the precision and coverage for each of the methods. Figure 2 illustrates how the single mean AP score (as reported ...  relative precision of different methods. In combination, Table 5 and Figure 2 demonstrate that the proposed methods extract instances about as ...  methods for returning empty lists, thus favoring the baseline (see Figure 2). estimates. For each of these labels, we manually check ..."</mention10>
<mention20>"... of instances returned for several class labels and Table 5 shows the precision and coverage for each of the methods. Figure 2 illustrates how the single mean AP score (as reported in Table 5) can misrepresent the relative precision of different methods. In combination, Table 5 and Figure 2 demonstrate that the proposed methods extract instances about as well as the baseline, whenever the baseline can extract anything ...  returned. "MAP" is mean average precision. MAP does not punish methods for returning empty lists, thus favoring the baseline (see Figure 2). estimates. For each of these labels, we manually check the top 10 instances proposed by each method to determine ..."</mention20>
<mention50>"... all the class labels. MAP is only computed over class labels for which the method returns something, meaning methods are not punished for returning empty lists. Table 4 gives examples of instances returned for several class labels and Table 5 shows the precision and coverage for each of the methods. Figure 2 illustrates how the single mean AP score (as reported in Table 5) can misrepresent the relative precision of different methods. In combination, Table 5 and Figure 2 demonstrate that the proposed methods extract instances about as well as the baseline, whenever the baseline can extract anything at all; i.e. the proposed method does not cause a precision drop on classes covered by the baseline. In addition, there are many classes for which the baseline is not ...  "Coverage" is the number of class labels (out of 100) for which at least one instance was returned, followed by the number for which at least one correct instance was returned. "MAP" is mean average precision. MAP does not punish methods for returning empty lists, thus favoring the baseline (see Figure 2). estimates. For each of these labels, we manually check the top 10 instances proposed by each method to determine whether each belongs to the class. Table 6 shows the precision scores for each method computed against the original Wikipedia list of instances and against our manually-augmented list of gold ..."</mention50>
<lines3> Table 4 gives examples of instances returned for several class labels and Table 5 shows the precision and coverage for each of the methods. Figure 2 illustrates how the single mean AP score (as reported in Table 5) can misrepresent the relative precision of different methods. 1n combination, Table 5 and Figure 2 demonstrate that the proposed methods extract instances about as well as the baseline, whenever the baseline can extract anything at all; i.e. the proposed method does not cause a precision drop on classes covered by the baseline. ...  "MAP" is mean average precision. MAP does not punish methods for returning empty lists, thus favoring the baseline (see Figure 2). estimates"</lines3>
<lines5> MAP is only computed over class labels for which the method returns something, meaning methods are not punished for returning empty lists. Table 4 gives examples of instances returned for several class labels and Table 5 shows the precision and coverage for each of the methods. Figure 2 illustrates how the single mean AP score (as reported in Table 5) can misrepresent the relative precision of different methods. 1n combination, Table 5 and Figure 2 demonstrate that the proposed methods extract instances about as well as the baseline, whenever the baseline can extract anything at all; i.e. the proposed method does not cause a precision drop on classes covered by the baseline. 1n addition, there are many classes for which the baseline is not able to extract any instances, but the proposed method is. ...  We choose class labels for which Hearst was able to return at least one instance, in order to ensure reliable precision Flemish still life painters: Clara Peeters · Willem Kalf · Jan Davidsz de Heem · Pieter Claesz · Peter Paul Rubens · Frans Snyders · Jan Brueghel the Elder · Hans Memling · Pieter Bruegel the Elder · Caravaggio · Abraham Brueghel Pakistani cricket captains:  "Coverage" is the number of class labels (out of 100) for which at least one instance was returned, followed by the number for which at least one correct instance was returned. "MAP" is mean average precision. MAP does not punish methods for returning empty lists, thus favoring the baseline (see Figure 2). estimates. For each of these labels, we manually check the top 10 instances proposed by each method to determine whether each belongs to the class"</lines5>
<snippet3> Table 4 gives examples of instances returned for several class labels and Table 5 shows the precision and coverage for each of the methods. Figure 2 illustrates how the single mean AP score (as reported in Table 5) can misrepresent the relative precision of different methods. 1n combination, Table 5 and Figure 2 demonstrate that the proposed methods extract instances about as well as the baseline, whenever the baseline can extract anything at all; i.e. the proposed method does not cause a precision drop on classes covered by the baseline.</snippet3>
<snippet5> MAP is only computed over class labels for which the method returns something, meaning methods are not punished for returning empty lists. Table 4 gives examples of instances returned for several class labels and Table 5 shows the precision and coverage for each of the methods. Figure 2 illustrates how the single mean AP score (as reported in Table 5) can misrepresent the relative precision of different methods. 1n combination, Table 5 and Figure 2 demonstrate that the proposed methods extract instances about as well as the baseline, whenever the baseline can extract anything at all; i.e. the proposed method does not cause a precision drop on classes covered by the baseline. 1n addition, there are many classes for which the baseline is not able to extract any instances, but the proposed method is.</snippet5>
<abstract>We present a method for populating fine-grained classes (e.g., &quot;1950s Amer-ican jazz musicians&quot;) with instances (e.g., Charles Mingus). While state-of-the-art methods tend to treat class labels as single lexical units, the proposed method considers each of the individual modifiers in the class label relative to the head. An evaluation on the task of reconstructing Wikipedia category pages demonstrates a &gt;10 point increase in AUC, over a strong baseline relying on widely-used Hearst patterns.</abstract>
<title>Identifying 1950s American Jazz Musicians: Fine-Grained IsA Extraction via Modifier Composition</title>
<introduction>The majority of approaches ( Snow et al., 2006;Shwartz et al., 2016) for extracting IsA relations from text rely on lexical patterns as the primary signal of whether an instance belongs to a class. For example, observing a pattern like "X such as Y" is a strong indication that Y (e.g., "Charles Mingus") is an instance of class X (e.g., "musician") (Hearst, 1992).Methods based on these "Hearst patterns" assume that class labels can be treated as atomic lexicalized units. This assumption has several significant weakness. First, in order to recognize an instance of a class, these patternbased methods require that the entire class label be observed verbatim in text. The requirement is reasonable for class labels containing a single word, but in practice, there are many possible fine-grained classes: not only "musicians" but also "1950s American jazz musicians". The probability that a given label will appear in its entirety within one of the expected patterns is very low, even in large * Contributed during an internship at Google.  amounts of text. Second, when class labels are treated as though they cannot be decomposed, every class label must be modeled independently, even those containing overlapping words ("American jazz musician", "French jazz musician"). As a result, the number of meaning representations to be learned is exponential in the length of the class label, and quickly becomes intractable. Thus, compositional models of taxonomic relations are necessary for better language understanding.We introduce a compositional approach for reasoning about fine-grained class labels. Our approach is based on the notion from formal semantics, in which modifiers ("1950s") correspond to properties that differentiate instances of a subclass ("1950s musicians") from instances of the superclass ("musicians") (Heim and Kratzer, 1998). Our method consists of two stages: interpreting each modifier relative to the head ("musicians active during 1950s"), and using the interpretations to identify instances of the class from text ( Figure  1). Our main contributions are: 1) a compositional method for IsA extraction, which in-volves a novel application of noun-phrase paraphrasing methods to the task of semantic taxonomy induction and 2) the operationalization of a formal semantics framework to address two aspects of semantics that are often kept separate in NLP: assigning intrinsic "meaning" to a phrase, and reasoning about that phrase in a truth-theoretic context.</introduction>
<file>P17-1192-Figure2-1.png</file>
<figure>3</figure>
<caption>Figure 3: ROC curves for selected methods (Hearst in blue, proposed in red). Given a ranked list of instances, ROC curves plot true positives vs. false positives retained by setting various cutoffs. The curve becomes linear once all remaining instances have the same score (e.g., 0), as this makes it impossible to add true positives without also including all remaining false positives.</caption>
<mention10>"... scored as 0. Table 7 reports the AUC and recall. Figure 3 plots the full ROC curves. The requirement by Hearst ..."</mention10>
<mention20>"... the score the method assigns, and every other instance is scored as 0. Table 7 reports the AUC and recall. Figure 3 plots the full ROC curves. The requirement by Hearst that class labels appear in full in a single sentence ..."</mention20>
<mention50>"... every instance of the head H as a candidate). Then, for each method, we rank this full set of candidates such that any instance returned by the method is given the score the method assigns, and every other instance is scored as 0. Table 7 reports the AUC and recall. Figure 3 plots the full ROC curves. The requirement by Hearst that class labels appear in full in a single sentence results in very low recall, which translates into very low AUC when considering the full set of candidate instances. In comparison, the proposed compositional methods make use of a larger ..."</mention50>
<lines3> Table 7 reports the AUC and recall. Figure 3 plots the full ROC curves. The requirement by Hearst that class labels appear in full in a single sentence results in very low recall, which translates into very low AUC when considering the full set of candidate instances"</lines3>
<lines5> Then, for each method, we rank this full set of candidates such that any instance returned by the method is given the score the method assigns, and every other instance is scored as 0. Table 7 reports the AUC and recall. Figure 3 plots the full ROC curves. The requirement by Hearst that class labels appear in full in a single sentence results in very low recall, which translates into very low AUC when considering the full set of candidate instances. 1n comparison, the proposed compositional methods make use of a larger set of sentences, and provide nonzero scores for many more candidates, resulting in a &gt;10 point increase in AUC on both UniformSet and WeightedSet (Table 7)"</lines5>
<snippet3> Table 7 reports the AUC and recall. Figure 3 plots the full ROC curves. The requirement by Hearst that class labels appear in full in a single sentence results in very low recall, which translates into very low AUC when considering the full set of candidate instances"</snippet3>
<snippet5> Then, for each method, we rank this full set of candidates such that any instance returned by the method is given the score the method assigns, and every other instance is scored as 0. Table 7 reports the AUC and recall. Figure 3 plots the full ROC curves. The requirement by Hearst that class labels appear in full in a single sentence results in very low recall, which translates into very low AUC when considering the full set of candidate instances. 1n comparison, the proposed compositional methods make use of a larger set of sentences, and provide nonzero scores for many more candidates, resulting in a &gt;10 point increase in AUC on both UniformSet and WeightedSet (Table 7)"</snippet5>
<abstract>We present a method for populating fine-grained classes (e.g., &quot;1950s Amer-ican jazz musicians&quot;) with instances (e.g., Charles Mingus). While state-of-the-art methods tend to treat class labels as single lexical units, the proposed method considers each of the individual modifiers in the class label relative to the head. An evaluation on the task of reconstructing Wikipedia category pages demonstrates a &gt;10 point increase in AUC, over a strong baseline relying on widely-used Hearst patterns.</abstract>
<title>Identifying 1950s American Jazz Musicians: Fine-Grained IsA Extraction via Modifier Composition</title>
<introduction>The majority of approaches ( Snow et al., 2006;Shwartz et al., 2016) for extracting IsA relations from text rely on lexical patterns as the primary signal of whether an instance belongs to a class. For example, observing a pattern like "X such as Y" is a strong indication that Y (e.g., "Charles Mingus") is an instance of class X (e.g., "musician") (Hearst, 1992).Methods based on these "Hearst patterns" assume that class labels can be treated as atomic lexicalized units. This assumption has several significant weakness. First, in order to recognize an instance of a class, these patternbased methods require that the entire class label be observed verbatim in text. The requirement is reasonable for class labels containing a single word, but in practice, there are many possible fine-grained classes: not only "musicians" but also "1950s American jazz musicians". The probability that a given label will appear in its entirety within one of the expected patterns is very low, even in large * Contributed during an internship at Google.  amounts of text. Second, when class labels are treated as though they cannot be decomposed, every class label must be modeled independently, even those containing overlapping words ("American jazz musician", "French jazz musician"). As a result, the number of meaning representations to be learned is exponential in the length of the class label, and quickly becomes intractable. Thus, compositional models of taxonomic relations are necessary for better language understanding.We introduce a compositional approach for reasoning about fine-grained class labels. Our approach is based on the notion from formal semantics, in which modifiers ("1950s") correspond to properties that differentiate instances of a subclass ("1950s musicians") from instances of the superclass ("musicians") (Heim and Kratzer, 1998). Our method consists of two stages: interpreting each modifier relative to the head ("musicians active during 1950s"), and using the interpretations to identify instances of the class from text ( Figure  1). Our main contributions are: 1) a compositional method for IsA extraction, which in-volves a novel application of noun-phrase paraphrasing methods to the task of semantic taxonomy induction and 2) the operationalization of a formal semantics framework to address two aspects of semantics that are often kept separate in NLP: assigning intrinsic "meaning" to a phrase, and reasoning about that phrase in a truth-theoretic context.</introduction>
<file>P17-1192-Figure3-1.png</file>
<figure>1</figure>
<caption>Figure 1: We extract instances of fine-grained classes by considering each of the modifiers in the class label individually. This allows us to extract instances even when the full class label never appears in text.</caption>
<mention10>"..."</mention10>
<mention20>"..."</mention20>
<mention50>"..."</mention50>
<lines3> Our approach is based on the notion from formal semantics, in which modifiers ("1950s") correspond to properties that differentiate instances of a subclass ("1950s musicians") from instances of the superclass ("musicians") (Heim and Kratzer, 1998). Our method consists of two stages: interpreting each modifier relative to the head ("musicians active during 1950s"), and using the interpretations to identify instances of the class from text ( Figure  1). Our main contributions are: 1) a compositional method for 1sA extraction, which in-volves a novel application of noun-phrase paraphrasing methods to the task of semantic taxonomy induction and 2) the operationalization of a formal semantics framework to address two aspects of semantics that are often kept separate in NLP: assigning intrinsic "meaning" to a phrase, and reasoning about that phrase in a truth-theoretic context"</lines3>
<lines5> We introduce a compositional approach for reasoning about fine-grained class labels. Our approach is based on the notion from formal semantics, in which modifiers ("1950s") correspond to properties that differentiate instances of a subclass ("1950s musicians") from instances of the superclass ("musicians") (Heim and Kratzer, 1998). Our method consists of two stages: interpreting each modifier relative to the head ("musicians active during 1950s"), and using the interpretations to identify instances of the class from text ( Figure  1). Our main contributions are: 1) a compositional method for 1sA extraction, which in-volves a novel application of noun-phrase paraphrasing methods to the task of semantic taxonomy induction and 2) the operationalization of a formal semantics framework to address two aspects of semantics that are often kept separate in NLP: assigning intrinsic "meaning" to a phrase, and reasoning about that phrase in a truth-theoretic context. Noun Phrase 1nterpretation"</lines5>
<snippet3> Our approach is based on the notion from formal semantics, in which modifiers ("1950s") correspond to properties that differentiate instances of a subclass ("1950s musicians") from instances of the superclass ("musicians") (Heim and Kratzer, 1998). Our method consists of two stages: interpreting each modifier relative to the head ("musicians active during 1950s"), and using the interpretations to identify instances of the class from text ( Figure  1). Our main contributions are: 1) a compositional method for 1sA extraction, which in-volves a novel application of noun-phrase paraphrasing methods to the task of semantic taxonomy induction and 2) the operationalization of a formal semantics framework to address two aspects of semantics that are often kept separate in NLP: assigning intrinsic "meaning" to a phrase, and reasoning about that phrase in a truth-theoretic context"</snippet3>
<snippet5> We introduce a compositional approach for reasoning about fine-grained class labels. Our approach is based on the notion from formal semantics, in which modifiers ("1950s") correspond to properties that differentiate instances of a subclass ("1950s musicians") from instances of the superclass ("musicians") (Heim and Kratzer, 1998). Our method consists of two stages: interpreting each modifier relative to the head ("musicians active during 1950s"), and using the interpretations to identify instances of the class from text ( Figure  1). Our main contributions are: 1) a compositional method for 1sA extraction, which in-volves a novel application of noun-phrase paraphrasing methods to the task of semantic taxonomy induction and 2) the operationalization of a formal semantics framework to address two aspects of semantics that are often kept separate in NLP: assigning intrinsic "meaning" to a phrase, and reasoning about that phrase in a truth-theoretic context. Noun Phrase 1nterpretation"</snippet5>
<abstract>We present a method for populating fine-grained classes (e.g., &quot;1950s Amer-ican jazz musicians&quot;) with instances (e.g., Charles Mingus). While state-of-the-art methods tend to treat class labels as single lexical units, the proposed method considers each of the individual modifiers in the class label relative to the head. An evaluation on the task of reconstructing Wikipedia category pages demonstrates a &gt;10 point increase in AUC, over a strong baseline relying on widely-used Hearst patterns.</abstract>
<title>Identifying 1950s American Jazz Musicians: Fine-Grained IsA Extraction via Modifier Composition</title>
<introduction>The majority of approaches ( Snow et al., 2006;Shwartz et al., 2016) for extracting IsA relations from text rely on lexical patterns as the primary signal of whether an instance belongs to a class. For example, observing a pattern like "X such as Y" is a strong indication that Y (e.g., "Charles Mingus") is an instance of class X (e.g., "musician") (Hearst, 1992).Methods based on these "Hearst patterns" assume that class labels can be treated as atomic lexicalized units. This assumption has several significant weakness. First, in order to recognize an instance of a class, these patternbased methods require that the entire class label be observed verbatim in text. The requirement is reasonable for class labels containing a single word, but in practice, there are many possible fine-grained classes: not only "musicians" but also "1950s American jazz musicians". The probability that a given label will appear in its entirety within one of the expected patterns is very low, even in large * Contributed during an internship at Google.  amounts of text. Second, when class labels are treated as though they cannot be decomposed, every class label must be modeled independently, even those containing overlapping words ("American jazz musician", "French jazz musician"). As a result, the number of meaning representations to be learned is exponential in the length of the class label, and quickly becomes intractable. Thus, compositional models of taxonomic relations are necessary for better language understanding.We introduce a compositional approach for reasoning about fine-grained class labels. Our approach is based on the notion from formal semantics, in which modifiers ("1950s") correspond to properties that differentiate instances of a subclass ("1950s musicians") from instances of the superclass ("musicians") (Heim and Kratzer, 1998). Our method consists of two stages: interpreting each modifier relative to the head ("musicians active during 1950s"), and using the interpretations to identify instances of the class from text ( Figure  1). Our main contributions are: 1) a compositional method for IsA extraction, which in-volves a novel application of noun-phrase paraphrasing methods to the task of semantic taxonomy induction and 2) the operationalization of a formal semantics framework to address two aspects of semantics that are often kept separate in NLP: assigning intrinsic "meaning" to a phrase, and reasoning about that phrase in a truth-theoretic context.</introduction>
<file>P17-1192-Figure1-1.png</file>