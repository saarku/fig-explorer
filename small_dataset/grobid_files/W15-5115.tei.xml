<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/skuzi2/acl_files/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2018-06-16T08:23+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Analysis of Dysarthric Speech using Distinctive Feature Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ka</forename><forename type="middle">Ho</forename><surname>Wong</surname></persName>
							<email>khwong@se.cuhk.edu.hk, ytyeung@se.cuhk.edu.hk, p.wong@cuhk.edu.hk, levow@uw.edu,</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Systems Engineering and Engineering Management</orgName>
								<orgName type="department" key="dep2">Data Decision Analytics Research Centre</orgName>
								<orgName type="laboratory">Human-Computer Communications Laboratory</orgName>
								<address>
									<addrLine>2 Stanley Ho Big</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Ting Yeung</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">C M</forename><surname>Wong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Mind and Brain</orgName>
								<orgName type="institution">CUHK-Utrecht University Centre for Language</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Linguistics and Modern Languages</orgName>
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">Hong Kong SAR</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gina-Anne</forename><surname>Levow</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Meng</surname></persName>
							<email>hmmeng@se.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Systems Engineering and Engineering Management</orgName>
								<orgName type="department" key="dep2">Data Decision Analytics Research Centre</orgName>
								<orgName type="laboratory">Human-Computer Communications Laboratory</orgName>
								<address>
									<addrLine>2 Stanley Ho Big</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Analysis of Dysarthric Speech using Distinctive Feature Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: speech recognition</term>
					<term>distinctive feature</term>
					<term>multi- layer perceptron</term>
					<term>dysarthric speech</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Imprecise articulatory breakdown is one of the characteristics of dysarthric speech. This work attempts to develop a framework to automatically identify problematic articulatory patterns of dysarthric speakers in terms of distinctive features (DFs), which are effective for describing speech production. The identification of problematic articulatory patterns aims to assist speech therapists in developing intervention strategies. A multilayer perceptron (MLP) system is trained with non-dysarthric speech data for DF recognition. Agreement rates between the recognized DF values and the canonical values based on phonetic transcriptions are computed. For non-dysarthric speech, our system achieves an average agreement rate of 85.7%. The agreement rate of dysarthric speech declines , ranging between 1% to 3% in mild cases, 4% to 7% in moderate cases, and 7% to 12% in severe cases, when compared with non-dysarthric speech. We observe that the DF disagreement patterns are consistent with the analysis of a speech therapist.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Dysarthria is a speech disorder caused by disturbances in the muscular control of the speech production mechanism <ref type="bibr" target="#b0">[1]</ref>. Stroke, Parkinson's disease, cerebral palsy, amyotrophic lateral sclerosis and others nervous system-related diseases may cause dysarthria. Dysarthria affects millions of adults around the world, especially their effective speech communication in daily life. Speech-related problems include respiration, phonation, articulation and resonance. Symptoms that emerge in speech signals include hoarseness in voice quality, imprecise segmental articulation, excessive nasalization, as well as disordered prosody. All are detrimental to speech intelligibility.</p><p>Treatment of dysarthria involves perceptual assessment to characterize the problematic articulatory patterns, devise intervention strategies and monitor progress. Speech therapists generally listen carefully to dysarthric speech, possibly multiple times, in order to monitor progress, and such a process is costly. The situation calls for data-driven, computational techniques that analyze the problematic articulatory patterns of dysarthric speakers, in an attempt to assist human efforts in analysis to inform the development of intervention strategies.</p><p>Articulatory features describe the place and manner of articulation in speech production. They have been well-studied in the context of speech technology development, articulatory feature recognition with multiplayer perceptrons (MLPs) in telephone speech <ref type="bibr" target="#b1">[2]</ref>, and articulatory feature recognizer for dysarthric speech using neural networks and support vector machines <ref type="bibr" target="#b2">[3]</ref>  <ref type="bibr" target="#b3">[4]</ref>. In particular, distinctive features (DFs) are a type of articulatory feature that also describe the general characteristics and acoustic consequences of the constrictions within the vocal tract <ref type="bibr" target="#b4">[5]</ref>. DF have been shown to be wellidentifiable from speech signals <ref type="bibr" target="#b4">[5]</ref>  <ref type="bibr" target="#b5">[6]</ref>, which motivates us to study the use of DFs in the analysis of dysarthric speech.</p><p>We aim to identify problematic articulatory patterns of dysarthric speech in terms of DFs. We apply an MLP-based DF recognition system on both dysarthric and non-dysarthric speech data from the TORGO corpus <ref type="bibr" target="#b6">[7]</ref>. We compare the DF recognition results between dysarthric and non-dysarthric speech, with the DF reference derived from canonical pronunciations. For dysarthric subjects, we observe that the agreement rates of the DFs corresponding to poor articulation are significantly lower than those of the non-dysarthric subjects. We also note the relationships between the problematic articulatory patterns and the lower agreement rates of the corresponding DFs.</p><p>In the next section, we discuss the dysarthric corpus used for this study. In Section 3, we describe the development of a DF recognition system and the procedures to utilize the recognition results. In Section 4, we compare the results between manual analysis of the data based on Frenchay Dysarthric Assessment (FDA) <ref type="bibr" target="#b7">[8]</ref> and the automatic DF recognition. We conclude our work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Dysarthric Speech</head><p>The TORGO (LDC2012S02) <ref type="bibr" target="#b6">[7]</ref> corpus is a dysarthric speech corpus. The corpus includes 8 dysarthric subjects (3 females and 5 males) and 7 non-dysarthric subjects (4 male and 3 females). 7 dysarthric subjects are cerebral palsy and 1 is amyotrophic lateral sclerosis. There are 5 types of tasks in TORGO: recording articulatory movement tasks such as repeating "Ah-P-Eee", picture description, actions such as relaxing the mouth in its normal position, single word utterances such as saying "yes" and sentential utterances such as "the quick brown fox jumps over the lazy dog". We focus on the single word tasks and sentence tasks. The dataset consists of 4,605 nondysarthric speech utterances and 2,518 dysarthric speech utterances <ref type="table">(Table 1</ref>). For the non-dysarthric speech, we further divide the data into a training set of 3,012 utterances and a testing set of 1,593 utterances. Both training and testing include male and female non-dysarthric subjects and no speakers overlap between training and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Distinctive Feature Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Phonetic-level Alignment of Speech Data</head><p>We perform automatic forced alignment on the TORGO speech data (both non-dysarthric and dysarthric) with the HTK toolkit <ref type="bibr" target="#b8">[9]</ref>. We obtain phonetic-level alignments according to canonical pronunciations. We adopt the TIMIT phone set with modifications on the stops and diphthongs as in <ref type="bibr" target="#b1">[2]</ref>. A stop like /p/ is split into a closure /pcl/ and release /p/. A diphthong is split into two phones. For example, /oy/ in "boy" is represented as the rounded portion /oy1/ followed by the unrounded portion /oy2/. We train an acoustic model based on the modified phone set with the TORGO non-dysarthric speech training dataset with the HTK scripts published in <ref type="bibr" target="#b9">[10]</ref>.</p><p>Phone deletion is observed in the dysarthric speech of the TORGO corpus as described in <ref type="bibr" target="#b10">[11]</ref>. For example, M01 deletes /h/ in the word "house". We apply constrained grammars to handle phone deletions as shown in <ref type="figure">Figure 1</ref>. The constrained grammars are based on the phonetic-level canonical transcriptions, but an optional deletion path is provided for each phone.</p><p>The current analysis is based on the "real" alignments which do not contain the deleted phones, although the statistics of phone deletion may be useful in future researches. An example of dysarthric speech alignment result is shown in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Distinctive Features</head><p>Phonemes in languages can be represented in terms of a vector of distinctive features (DF) that capture their characteristics <ref type="bibr" target="#b5">[6]</ref>. DFs include articulator-bound features like high, back, which relate to the tongue. DFs also include articulator-free features, such as tense, which correspond to the level of articulatory movement. We allow three possible values for each DF: positive ("+"), negative ("-") and "don't care" ("*"). "Positive" means that the articulatory movement that produces the phoneme fit the definition of the DF. For example, nasal is positive for /m/, which indicates that when /m/ is produced, the soft palate is lowered. "Negative" means that the articulatory movement and acoustic consequences described by the DF must not be observed when the phoneme is produced. For example, /b/ must be un-aspirated ("-"). Otherwise, it will become /p/ ("+" aspirated). "Don't care" means that the DF is not distinctive to the phone (e.g., high in /p/), or irrelevant (e.g. tense for /p/). We have chosen to apply 21 DFs in this work and their brief definitions are listed in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>DFs describe specific articulatory movements in speech production and their acoustic consequences. When DFs are applied for analysis of dysarthric speech, they should be able to help identify the problematic articulatory patterns that can inform the development of intervention strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">DF Recognition with Multilayer Perception</head><p>To train a DF recognition system, we start from the nondysarthric speech data from the TIMIT training set. The   Tense vowels are more intense, of longer duration and articulated with a greater deviation of the vocal cavity from its rest position then the lax vowels Delayed Release <ref type="bibr" target="#b19">[20]</ref> Slow release of stop closure</p><formula xml:id="formula_0">Consonantal [6]</formula><p>The absence or modification of constrictions in oral cavity Continuant <ref type="bibr" target="#b5">[6]</ref> Forming of complete closure   <ref type="figure">Figure 3</ref>: An example of substitution --/sh/ /t/. "*" means "don't care". The shaded regions represent the outputs that we are interested. "L" and "R" mean labelled and recognized values respectively. "X" shows how the tense value being recognized in two settings. Since the tense value in /sh/ is "*", we don't care it being recognized as "*" (a) or "-" (b)</p><p>TIMIT (LDC93S1) <ref type="bibr" target="#b11">[12]</ref> corpus is a non-dysarthric speech corpus from a wide variety of speakers. The corpus provides us 6,300 non-dysarthric utterances for initial model training. It contains phonetic-level transcriptions with manually adjusted time alignment. We train a frame-based MLP classifier for each of the 21 DFs <ref type="bibr" target="#b12">[13]</ref>. Each MLP classifier consists of three hidden layers with 50 x 12 x 50 units in the hidden layers and sigmoid activation based on the previous work <ref type="bibr" target="#b13">[14]</ref>. For the input layer, each input feature vector consists of features from 9 consecutive frames centered on the frame of interest to include the left-right context <ref type="bibr" target="#b1">[2]</ref>. For each frame, the feature is 39-dimenensional Mel-frequency cepstral coefficients (MFCC) (12 coefficients + log-energy + Δ + ΔΔ). The feature is normalized as zero mean and unit variance.</p><p>At the output layer, there are two possible configurations, either (a) with three-class "+", "-" or "don't care", or (b) with two-class "+" or "-". The different configurations have different confusion matrices <ref type="figure">(Figure 3)</ref>. We choose the two-class configuration (b) as in <ref type="figure">Figure 3</ref>. The DF recognition problem is generally a binary decision problem as to whether the recognized value matches with the reference value. For a case labeled "don't care", it is irrelevant whether the classifier's output is "+" or "-", because the DF value does not affect the phone's identity. During the training of each DF, we skip the frames which are silent or labeled as "don't care", but we still include them into the feature vectors. The label with maximum posterior probability will be assigned to the frame <ref type="bibr" target="#b11">[12]</ref>.</p><p>We further adapt the TIMIT MLP classifiers with nondysarthric speech data of the TORGO corpus. The initial weights of the adapted classifiers are the same as the weights in the TIMIT MLP classifiers. The weights are updated with the same training process.</p><p>During DF recognition, we apply all 21 DF classifiers on both dysarthric and non-dysarthric speech data to obtain the corresponding DF values ("+" or "-") at each frame. For the TIMIT corpus, we compare the recognized DF results with real transcriptions included in the corpus. For the TORGO corpus, we compare the results with the canonical DF transcriptions by assuming that the subjects intend to read the prompts correctly. This is appropriate for a real application where real transcriptions are not available immediately. We thus interpret the recognized results as the agreement rate between the recognition system and the canonical DF transcriptions. In computing the agreement rate of each DF, we only consider the frame situated at the middle of the start time and end time of a phone. <ref type="figure" target="#fig_1">Figure 4</ref> shows the performance of each DF on the TIMIT testing set with the TIMIT MLP recognition system. An average agreement rate of 91.9% suggests that the DF recognizer is well-trained with non-dysarthric speech, as compared with 92% average frame on phonological binary features achieved by <ref type="bibr" target="#b14">[15]</ref>. <ref type="figure" target="#fig_1">Figure 4</ref> also shows the performance of the adapted DF recognition system on the TORGO dysarthric and nondysarthric speech data. On non-dysarthric speech of the TORGO corpus, the average agreement rate drops to about 85.7%. The slightly lower DF agreement rate of TORGO nondysarthric speech is probably due to occasional pronunciation variation from canonical pronunciations.</p><p>The severity of each dysarthric subject is reported in <ref type="bibr" target="#b10">[11]</ref>. The average reduction in agreement rates of each dysarthric subject is calculated by equation (1)</p><formula xml:id="formula_1">= ∑ − ,<label>(1)</label></formula><p>where D i is the average agreement rate reduction of dysarthric subject i, N is the total number of DFs, T j is the average    <ref type="table">Table 3</ref>: A comparison of severity and the average DF agreement rate degradation of individual subjects.</p><p>agreement rate of DF j from all non-dysarthric subjects in TORGO shown in <ref type="figure" target="#fig_1">Figure 4</ref>, A i,j is the agreement rate of DF j of dysarthric subject i. The average reduction in DF agreement rates, D i , is shown in <ref type="table">Table 3</ref>. More severely dysarthric subjects have larger agreement rate reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion on Dysarthric Speech</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Manual Analysis</head><p>A speech therapist has evaluated the severity of the dysarthric subjects in the TORGO corpus with Frenchay Dysarthric Assessment (FDA) <ref type="bibr" target="#b7">[8]</ref>. FDA is one of the standard dysarthric speech assessments and includes 28 tests for different articulations. Each test is rated from "no abnormality" to "severe". For speech production, there are tests of respiration, lips, jaw, palate, laryngeal production and tongue. There are also speech intelligibility tests at word, sentence and conversational levels. The FDA results provide us the reference to the severity of the dysarthric subjects on different articulatory dimensions.</p><p>We validate the recognized DF error patterns to the FDA results and the manual analysis from <ref type="bibr" target="#b10">[11]</ref>. In <ref type="bibr" target="#b10">[11]</ref>, the authors studied 25% of the speech data of each dysarthric subject and identified the pronunciation error patterns of the individual subjects. <ref type="figure">Figure 5</ref> shows the drop in DF agreement rates for two severely dysarthric subjects (F01 and M04), one moderately dysarthric subject (F03), one mildly dysarthric subject (M03) and two non-dysarthric subjects (FC02 and MC04) for comparison to illustrate the relationship among the error patterns and agreement rates. FC02's pronunciation is slightly better than that of MC04.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Severely Dysarthric Subjects</head><p>For the tongue-related DFs, F01 exhibits substantial drops in agreement rates on anterior, alveolar and velar. M04 also exhibits drops in agreement rates on high, front, back, anterior and alveolar relative to mildly dysarthric subjects. For F01 and M04, the speech therapist rated the correctness of articulation points and laboriousness of tongue motion as moderateto-severe. This result is consistent with the reduction of tongue-related DFs agreement rates.</p><p>F01 and M04 also exhibit drops in agreement rates on rounded and labial respectively. Both of them are diagnosed with consistently poor lip movements by the speech therapist. Both of them have relatively poor DF agreement rates on nasal compared to mild subjects. The speech therapist also remarked that F01 has nasal emission problems. Although the DF results show M04 also has difficulty with nasal, the speech therapist reported that M04 only had slight problems with soft palate movement. Further analysis is necessary.</p><p>The DF results on voiced suggest that F01 and M04 may have problems in laryngeal production. In <ref type="bibr" target="#b10">[11]</ref>, the authors observed that the two subjects voice voiceless target consonants (prevocalic voicing problems). This observation agrees with the speech therapist's findings that their voice production is inappropriate and ineffective in most situations.</p><p>For articulator-free DFs, the dysarthric subjects generally exhibit lower agreement rates on consonantal, continuant and strident. The trend is consistent with other consonant-related DFs. Continuant relates to the production of /f/ ("+", no com- <ref type="figure">Figure 5</ref>: The difference between the average DF agreement rate from the control subjects and the corresponding DF agreement rate of each dysarthric subject. The agreement rates of most of the DF drop substainally for severely dysarthric subjects. The agreement rates in moderately and mildly dysarthric subjects only dropped in a few DFs.  plete closure) and /p/ ("-", complete closure). The drop in continuant agreement rates of F01, M04 and F03 are higher than M03. The analysis in <ref type="bibr" target="#b10">[11]</ref> also found that some fricatives (e.g. /f/) are replaced with stops (e.g. /p/) by F01 and F03 but not by M04. Strident affects fricatives such /f/ and /s/. In <ref type="bibr" target="#b10">[11]</ref>, the authors observed that F01 and M04 replace fricatives such as /f/, /s/ with non-fricatives such as /p/, /t/. We also observe the large agreement rate reductions on strident for F01 and M04. There are substantial agreement rate reductions of sonorant for F01 and M04 (19.0% and 15.7% respectively). The results show that the subjects may have difficulty in building up pressure behind the constriction, which may be related to the lips problems described before.</p><p>Not all DFs exhibit these drops in agreement. The agreement rates on dental are similar among different dysarthric subjects. Some DFs may not be as useful in indicating the severity of the subjects. This is an area for future investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Mildly and Moderately Dysarthric Subjects</head><p>The mildly dysarthric and moderately dysarthric subjects (M03 and F03) only exhibit slight agreement rate reductions for most DFs. In terms of DF results, the average agreement rates of F03 are lower than M03. The observation agrees with <ref type="bibr" target="#b10">[11]</ref> that F03 is moderately dysarthric and M03 is mildly dysarthric. For F03, the agreement rates of tongue related DFs are worse than other articulator-bound DFs. The speech therapist also found that F03 had mild tongue-related problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>We compared the recognized DFs on dysarthric speech to prior results of manual analysis on the same dysarthric speech corpus. The general trends of reduced agreement are consistent with the analysis of the speech therapist and the observations of <ref type="bibr" target="#b10">[11]</ref>. This indicates a potential way to automate analysis of dysarthric speech to assistant speech therapists for the development of intervention strategies. In the future, we plan to extend this framework to other languages such as Chinese. We will continue to improve the DF recognition system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: An example of a constrained grammar to handle phone deletion. The optional phones are braced by squared brackets [].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The agreement rate of each DF between recognized results and canonical DFs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The 21 DFs and their brief descriptions. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>This project is partially sponsored by a grant from the Hong Kong SAR Government General Research Fund (reference no. GRF415513).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Motor Speech Disorders: Diagnosis &amp; Treatment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Freed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<pubPlace>Clifton Park, NY: Delmar, Cengage Learning</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Articulatory Feature Classifiers Trained on 2000 hours of Telephone Speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frankel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Magimai-Doss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Cetin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Combining Phonological and Acoustic ASR-free Features for Pathological Speech Intelligibility Assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Middag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bocklet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nöth</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interspeech</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Phonological Features in Discriminative Classification of Dysarthric Speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rudzicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustic, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Toward a Model for Lexical Access Based on Acoustic Landmarks and Distinctive Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Stevens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1872" to="91" />
			<date type="published" when="2002-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Acoustic Phonetic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Stevens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The TORGO Database of Acoustic and Articulatory Speech from Speakers with Dysarthric Patient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rudzicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Namasivayam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="523" to="541" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Frenchay Dysarthria Assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Enderby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
			<publisher>College Hill Press</publisher>
			<pubPlace>San Diego</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Odell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ollason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Valthcey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Woodland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
		<respStmt>
			<orgName>The HTK Book.: Cambridge University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Prosodylab-Aligner: A Tool for Forced Alignment of Laboratory Speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gorman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Howell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canadian Acoustics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1192" to="193" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adapting Acoustic and Lexical Models to Dysarthric Speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mengistu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rudzicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of Acoustic, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">TIMIT Acoustic-Phonetic Continuous Speech Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Garofolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lingusistic Data Consortium</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">ICSI QuickNet Software Package</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johnson</surname></persName>
		</author>
		<ptr target="http://www1.icsi.berkeley.edu/Speech/qn.html" />
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic Discovery of a Phonetic Inventory for Unwritten Languages for Statistical Speech Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Muthukumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Internationl Conference of Acoustic, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Detection of Phonological Features in Continuous Speech using Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="333" to="353" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A Course in Phonetics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ladefoged</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Wadsworth, Cengage Learning</publisher>
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">December) Phonetics and Phonlogy: Distinctive Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mannel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Problem Book in Phonology: A Workbook for Introductory Courses in Linguistics and in Modern Phonology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">N</forename><surname>Clements</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The Phonetics and Phonology of Retroflexes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Hamann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<pubPlace>Utrecht, The Netherlands, PhD Dissertion</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The Sound Pattern of English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chomsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
			<publisher>Harper &amp; Row</publisher>
			<pubPlace>NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Developing automatic articulation, phonation and accent assessment techniques for speakers treated for advanced head and neck cancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Middag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hilgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Den Brekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clapham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="44" to="54" />
			<date type="published" when="2014-01" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
